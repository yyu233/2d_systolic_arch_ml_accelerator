{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "ResNet_Cifar(\n",
      "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        8, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(\n",
      "          8, 32, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (weight_quant): weight_quantize_fn()\n",
      "        )\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(\n",
      "          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (weight_quant): weight_quantize_fn()\n",
      "        )\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "     \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "model_name = \"resnet20_quant4bit\"\n",
    "model = resnet20_quant()\n",
    "#model.conv1 = nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#model.bn1 = bn1 = nn.BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "model.layer1[0].conv1 = QuantConv2d(8, 8, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#model.layer1[0].conv2 = QuantConv2d(8, 8, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#model.layer1[0].bn1 = nn.BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "model.layer1[0].bn2 = nn.Sequential()\n",
    "#model.layer1[1].conv1 = QuantConv2d(8, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [80, 120]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "junior-reminder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 0.450 (0.450)\tData 0.264 (0.264)\tLoss 2.3541 (2.3541)\tPrec 11.719% (11.719%)\n",
      "Epoch: [0][100/391]\tTime 0.085 (0.085)\tData 0.002 (0.005)\tLoss 1.8977 (2.0458)\tPrec 28.125% (21.968%)\n",
      "Epoch: [0][200/391]\tTime 0.079 (0.078)\tData 0.002 (0.004)\tLoss 1.6753 (1.8851)\tPrec 31.250% (28.148%)\n",
      "Epoch: [0][300/391]\tTime 0.087 (0.080)\tData 0.002 (0.003)\tLoss 1.5059 (1.7835)\tPrec 43.750% (32.345%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.243 (0.243)\tLoss 1.4584 (1.4584)\tPrec 47.656% (47.656%)\n",
      " * Prec 46.960% \n",
      "best acc: 46.960000\n",
      "Epoch: [1][0/391]\tTime 0.273 (0.273)\tData 0.227 (0.227)\tLoss 1.3085 (1.3085)\tPrec 51.562% (51.562%)\n",
      "Epoch: [1][100/391]\tTime 0.081 (0.086)\tData 0.002 (0.005)\tLoss 1.3366 (1.3863)\tPrec 49.219% (48.933%)\n",
      "Epoch: [1][200/391]\tTime 0.075 (0.079)\tData 0.003 (0.003)\tLoss 1.1047 (1.3530)\tPrec 60.938% (50.412%)\n",
      "Epoch: [1][300/391]\tTime 0.078 (0.080)\tData 0.002 (0.003)\tLoss 1.1065 (1.3178)\tPrec 57.812% (51.936%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.229 (0.229)\tLoss 1.2691 (1.2691)\tPrec 52.344% (52.344%)\n",
      " * Prec 54.950% \n",
      "best acc: 54.950000\n",
      "Epoch: [2][0/391]\tTime 0.289 (0.289)\tData 0.229 (0.229)\tLoss 1.0396 (1.0396)\tPrec 60.156% (60.156%)\n",
      "Epoch: [2][100/391]\tTime 0.088 (0.085)\tData 0.002 (0.004)\tLoss 1.2600 (1.1594)\tPrec 50.000% (58.424%)\n",
      "Epoch: [2][200/391]\tTime 0.068 (0.082)\tData 0.002 (0.003)\tLoss 1.0502 (1.1352)\tPrec 57.031% (59.297%)\n",
      "Epoch: [2][300/391]\tTime 0.089 (0.080)\tData 0.002 (0.003)\tLoss 0.9294 (1.1147)\tPrec 69.531% (60.089%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.243 (0.243)\tLoss 1.1025 (1.1025)\tPrec 62.500% (62.500%)\n",
      " * Prec 59.620% \n",
      "best acc: 59.620000\n",
      "Epoch: [3][0/391]\tTime 0.308 (0.308)\tData 0.234 (0.234)\tLoss 0.9238 (0.9238)\tPrec 67.969% (67.969%)\n",
      "Epoch: [3][100/391]\tTime 0.084 (0.086)\tData 0.002 (0.005)\tLoss 1.0357 (0.9956)\tPrec 57.812% (64.387%)\n",
      "Epoch: [3][200/391]\tTime 0.084 (0.085)\tData 0.002 (0.003)\tLoss 0.9527 (0.9808)\tPrec 70.312% (65.174%)\n",
      "Epoch: [3][300/391]\tTime 0.078 (0.076)\tData 0.002 (0.003)\tLoss 0.9287 (0.9648)\tPrec 65.625% (65.752%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.264 (0.264)\tLoss 1.0414 (1.0414)\tPrec 60.938% (60.938%)\n",
      " * Prec 65.060% \n",
      "best acc: 65.060000\n",
      "Epoch: [4][0/391]\tTime 0.314 (0.314)\tData 0.251 (0.251)\tLoss 0.8392 (0.8392)\tPrec 71.094% (71.094%)\n",
      "Epoch: [4][100/391]\tTime 0.084 (0.085)\tData 0.003 (0.005)\tLoss 1.0583 (0.8793)\tPrec 64.062% (69.067%)\n",
      "Epoch: [4][200/391]\tTime 0.089 (0.085)\tData 0.002 (0.004)\tLoss 0.7967 (0.8728)\tPrec 75.781% (69.232%)\n",
      "Epoch: [4][300/391]\tTime 0.070 (0.082)\tData 0.002 (0.003)\tLoss 0.7548 (0.8588)\tPrec 68.750% (69.786%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.219 (0.219)\tLoss 0.8806 (0.8806)\tPrec 70.312% (70.312%)\n",
      " * Prec 67.020% \n",
      "best acc: 67.020000\n",
      "Epoch: [5][0/391]\tTime 0.323 (0.323)\tData 0.238 (0.238)\tLoss 0.8300 (0.8300)\tPrec 71.094% (71.094%)\n",
      "Epoch: [5][100/391]\tTime 0.082 (0.086)\tData 0.003 (0.005)\tLoss 0.8147 (0.8115)\tPrec 69.531% (71.086%)\n",
      "Epoch: [5][200/391]\tTime 0.082 (0.085)\tData 0.003 (0.003)\tLoss 0.7742 (0.7981)\tPrec 75.000% (71.937%)\n",
      "Epoch: [5][300/391]\tTime 0.069 (0.084)\tData 0.002 (0.003)\tLoss 0.7925 (0.7860)\tPrec 71.875% (72.410%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.253 (0.253)\tLoss 0.7108 (0.7108)\tPrec 78.906% (78.906%)\n",
      " * Prec 72.120% \n",
      "best acc: 72.120000\n",
      "Epoch: [6][0/391]\tTime 0.355 (0.355)\tData 0.286 (0.286)\tLoss 0.6080 (0.6080)\tPrec 81.250% (81.250%)\n",
      "Epoch: [6][100/391]\tTime 0.088 (0.087)\tData 0.002 (0.005)\tLoss 0.8923 (0.7237)\tPrec 74.219% (74.667%)\n",
      "Epoch: [6][200/391]\tTime 0.085 (0.084)\tData 0.003 (0.004)\tLoss 0.6442 (0.7351)\tPrec 79.688% (74.460%)\n",
      "Epoch: [6][300/391]\tTime 0.089 (0.084)\tData 0.002 (0.003)\tLoss 0.6893 (0.7318)\tPrec 75.000% (74.481%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.286 (0.286)\tLoss 0.7381 (0.7381)\tPrec 75.000% (75.000%)\n",
      " * Prec 71.990% \n",
      "best acc: 72.120000\n",
      "Epoch: [7][0/391]\tTime 0.296 (0.296)\tData 0.227 (0.227)\tLoss 0.6461 (0.6461)\tPrec 76.562% (76.562%)\n",
      "Epoch: [7][100/391]\tTime 0.082 (0.086)\tData 0.002 (0.004)\tLoss 0.7118 (0.6964)\tPrec 76.562% (75.665%)\n",
      "Epoch: [7][200/391]\tTime 0.084 (0.085)\tData 0.003 (0.003)\tLoss 0.6497 (0.6968)\tPrec 77.344% (75.665%)\n",
      "Epoch: [7][300/391]\tTime 0.085 (0.084)\tData 0.003 (0.003)\tLoss 0.7227 (0.6926)\tPrec 75.000% (75.893%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.235 (0.235)\tLoss 0.7074 (0.7074)\tPrec 77.344% (77.344%)\n",
      " * Prec 73.020% \n",
      "best acc: 73.020000\n",
      "Epoch: [8][0/391]\tTime 0.300 (0.300)\tData 0.229 (0.229)\tLoss 0.5990 (0.5990)\tPrec 80.469% (80.469%)\n",
      "Epoch: [8][100/391]\tTime 0.083 (0.087)\tData 0.002 (0.005)\tLoss 0.7297 (0.6294)\tPrec 73.438% (77.847%)\n",
      "Epoch: [8][200/391]\tTime 0.090 (0.086)\tData 0.002 (0.003)\tLoss 0.5417 (0.6407)\tPrec 82.812% (77.585%)\n",
      "Epoch: [8][300/391]\tTime 0.080 (0.084)\tData 0.002 (0.003)\tLoss 0.6537 (0.6496)\tPrec 78.906% (77.237%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.254 (0.254)\tLoss 0.5947 (0.5947)\tPrec 79.688% (79.688%)\n",
      " * Prec 73.310% \n",
      "best acc: 73.310000\n",
      "Epoch: [9][0/391]\tTime 0.310 (0.310)\tData 0.220 (0.220)\tLoss 0.6461 (0.6461)\tPrec 77.344% (77.344%)\n",
      "Epoch: [9][100/391]\tTime 0.085 (0.084)\tData 0.003 (0.004)\tLoss 0.5616 (0.6200)\tPrec 78.125% (78.133%)\n",
      "Epoch: [9][200/391]\tTime 0.077 (0.081)\tData 0.001 (0.003)\tLoss 0.5922 (0.6225)\tPrec 82.812% (78.071%)\n",
      "Epoch: [9][300/391]\tTime 0.089 (0.081)\tData 0.002 (0.003)\tLoss 0.5848 (0.6184)\tPrec 79.688% (78.296%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.248 (0.248)\tLoss 0.6495 (0.6495)\tPrec 79.688% (79.688%)\n",
      " * Prec 74.850% \n",
      "best acc: 74.850000\n",
      "Epoch: [10][0/391]\tTime 0.285 (0.285)\tData 0.216 (0.216)\tLoss 0.5345 (0.5345)\tPrec 81.250% (81.250%)\n",
      "Epoch: [10][100/391]\tTime 0.083 (0.072)\tData 0.002 (0.004)\tLoss 0.5330 (0.5848)\tPrec 82.812% (79.394%)\n",
      "Epoch: [10][200/391]\tTime 0.085 (0.078)\tData 0.002 (0.003)\tLoss 0.5433 (0.5814)\tPrec 80.469% (79.680%)\n",
      "Epoch: [10][300/391]\tTime 0.080 (0.080)\tData 0.002 (0.003)\tLoss 0.6818 (0.5862)\tPrec 78.906% (79.540%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.228 (0.228)\tLoss 0.8302 (0.8302)\tPrec 70.312% (70.312%)\n",
      " * Prec 73.700% \n",
      "best acc: 74.850000\n",
      "Epoch: [11][0/391]\tTime 0.289 (0.289)\tData 0.229 (0.229)\tLoss 0.5822 (0.5822)\tPrec 78.125% (78.125%)\n",
      "Epoch: [11][100/391]\tTime 0.078 (0.076)\tData 0.003 (0.004)\tLoss 0.5536 (0.5864)\tPrec 82.031% (79.479%)\n",
      "Epoch: [11][200/391]\tTime 0.083 (0.080)\tData 0.002 (0.003)\tLoss 0.6443 (0.5869)\tPrec 78.125% (79.563%)\n",
      "Epoch: [11][300/391]\tTime 0.079 (0.082)\tData 0.002 (0.003)\tLoss 0.6508 (0.5821)\tPrec 76.562% (79.752%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.248 (0.248)\tLoss 0.5820 (0.5820)\tPrec 82.812% (82.812%)\n",
      " * Prec 78.340% \n",
      "best acc: 78.340000\n",
      "Epoch: [12][0/391]\tTime 0.289 (0.289)\tData 0.229 (0.229)\tLoss 0.4267 (0.4267)\tPrec 84.375% (84.375%)\n",
      "Epoch: [12][100/391]\tTime 0.065 (0.082)\tData 0.002 (0.004)\tLoss 0.4738 (0.5591)\tPrec 82.031% (80.569%)\n",
      "Epoch: [12][200/391]\tTime 0.084 (0.080)\tData 0.002 (0.003)\tLoss 0.6300 (0.5601)\tPrec 78.906% (80.352%)\n",
      "Epoch: [12][300/391]\tTime 0.087 (0.081)\tData 0.002 (0.003)\tLoss 0.5176 (0.5596)\tPrec 82.031% (80.425%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.252 (0.252)\tLoss 0.6737 (0.6737)\tPrec 78.125% (78.125%)\n",
      " * Prec 76.200% \n",
      "best acc: 78.340000\n",
      "Epoch: [13][0/391]\tTime 0.326 (0.326)\tData 0.270 (0.270)\tLoss 0.4763 (0.4763)\tPrec 83.594% (83.594%)\n",
      "Epoch: [13][100/391]\tTime 0.084 (0.087)\tData 0.002 (0.005)\tLoss 0.5228 (0.5415)\tPrec 82.031% (81.227%)\n",
      "Epoch: [13][200/391]\tTime 0.083 (0.074)\tData 0.002 (0.004)\tLoss 0.5844 (0.5446)\tPrec 82.031% (80.993%)\n",
      "Epoch: [13][300/391]\tTime 0.083 (0.077)\tData 0.002 (0.003)\tLoss 0.5250 (0.5456)\tPrec 80.469% (80.954%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.248 (0.248)\tLoss 0.6670 (0.6670)\tPrec 81.250% (81.250%)\n",
      " * Prec 76.210% \n",
      "best acc: 78.340000\n",
      "Epoch: [14][0/391]\tTime 0.293 (0.293)\tData 0.215 (0.215)\tLoss 0.4557 (0.4557)\tPrec 84.375% (84.375%)\n",
      "Epoch: [14][100/391]\tTime 0.084 (0.087)\tData 0.002 (0.004)\tLoss 0.5058 (0.5100)\tPrec 83.594% (82.472%)\n",
      "Epoch: [14][200/391]\tTime 0.073 (0.081)\tData 0.003 (0.003)\tLoss 0.5247 (0.5206)\tPrec 82.812% (81.965%)\n",
      "Epoch: [14][300/391]\tTime 0.085 (0.081)\tData 0.002 (0.003)\tLoss 0.5577 (0.5236)\tPrec 76.562% (81.824%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.244 (0.244)\tLoss 0.6220 (0.6220)\tPrec 76.562% (76.562%)\n",
      " * Prec 77.970% \n",
      "best acc: 78.340000\n",
      "Epoch: [15][0/391]\tTime 0.300 (0.300)\tData 0.228 (0.228)\tLoss 0.4235 (0.4235)\tPrec 85.938% (85.938%)\n",
      "Epoch: [15][100/391]\tTime 0.088 (0.087)\tData 0.002 (0.005)\tLoss 0.5197 (0.5056)\tPrec 82.812% (82.820%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][200/391]\tTime 0.065 (0.085)\tData 0.002 (0.003)\tLoss 0.4800 (0.5088)\tPrec 84.375% (82.521%)\n",
      "Epoch: [15][300/391]\tTime 0.089 (0.082)\tData 0.002 (0.003)\tLoss 0.4534 (0.5084)\tPrec 82.812% (82.478%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.281 (0.281)\tLoss 0.5133 (0.5133)\tPrec 82.812% (82.812%)\n",
      " * Prec 81.150% \n",
      "best acc: 81.150000\n",
      "Epoch: [16][0/391]\tTime 0.294 (0.294)\tData 0.221 (0.221)\tLoss 0.5418 (0.5418)\tPrec 79.688% (79.688%)\n",
      "Epoch: [16][100/391]\tTime 0.085 (0.086)\tData 0.002 (0.004)\tLoss 0.5579 (0.4868)\tPrec 80.469% (82.782%)\n",
      "Epoch: [16][200/391]\tTime 0.087 (0.083)\tData 0.003 (0.003)\tLoss 0.4721 (0.4972)\tPrec 85.938% (82.774%)\n",
      "Epoch: [16][300/391]\tTime 0.079 (0.079)\tData 0.002 (0.003)\tLoss 0.4302 (0.4969)\tPrec 83.594% (82.742%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.246 (0.246)\tLoss 0.7624 (0.7624)\tPrec 74.219% (74.219%)\n",
      " * Prec 76.860% \n",
      "best acc: 81.150000\n",
      "Epoch: [17][0/391]\tTime 0.393 (0.393)\tData 0.313 (0.313)\tLoss 0.6065 (0.6065)\tPrec 80.469% (80.469%)\n",
      "Epoch: [17][100/391]\tTime 0.089 (0.085)\tData 0.002 (0.005)\tLoss 0.5875 (0.4897)\tPrec 78.906% (82.959%)\n",
      "Epoch: [17][200/391]\tTime 0.087 (0.084)\tData 0.003 (0.004)\tLoss 0.5316 (0.4859)\tPrec 82.031% (83.162%)\n",
      "Epoch: [17][300/391]\tTime 0.065 (0.082)\tData 0.003 (0.003)\tLoss 0.4971 (0.4862)\tPrec 84.375% (83.191%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.229 (0.229)\tLoss 0.5865 (0.5865)\tPrec 82.812% (82.812%)\n",
      " * Prec 80.340% \n",
      "best acc: 81.150000\n",
      "Epoch: [18][0/391]\tTime 0.306 (0.306)\tData 0.246 (0.246)\tLoss 0.3910 (0.3910)\tPrec 85.938% (85.938%)\n",
      "Epoch: [18][100/391]\tTime 0.081 (0.084)\tData 0.002 (0.004)\tLoss 0.5331 (0.4713)\tPrec 79.688% (83.408%)\n",
      "Epoch: [18][200/391]\tTime 0.082 (0.084)\tData 0.002 (0.003)\tLoss 0.4758 (0.4747)\tPrec 80.469% (83.190%)\n",
      "Epoch: [18][300/391]\tTime 0.084 (0.083)\tData 0.002 (0.003)\tLoss 0.4410 (0.4767)\tPrec 85.156% (83.223%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.235 (0.235)\tLoss 0.6056 (0.6056)\tPrec 77.344% (77.344%)\n",
      " * Prec 77.970% \n",
      "best acc: 81.150000\n",
      "Epoch: [19][0/391]\tTime 0.266 (0.266)\tData 0.220 (0.220)\tLoss 0.4607 (0.4607)\tPrec 84.375% (84.375%)\n",
      "Epoch: [19][100/391]\tTime 0.085 (0.084)\tData 0.002 (0.004)\tLoss 0.4838 (0.4597)\tPrec 86.719% (84.213%)\n",
      "Epoch: [19][200/391]\tTime 0.084 (0.084)\tData 0.006 (0.003)\tLoss 0.5364 (0.4641)\tPrec 83.594% (83.936%)\n",
      "Epoch: [19][300/391]\tTime 0.074 (0.084)\tData 0.002 (0.003)\tLoss 0.4432 (0.4619)\tPrec 85.156% (84.056%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.221 (0.221)\tLoss 0.5757 (0.5757)\tPrec 80.469% (80.469%)\n",
      " * Prec 77.170% \n",
      "best acc: 81.150000\n",
      "Epoch: [20][0/391]\tTime 0.273 (0.273)\tData 0.206 (0.206)\tLoss 0.4832 (0.4832)\tPrec 79.688% (79.688%)\n",
      "Epoch: [20][100/391]\tTime 0.080 (0.084)\tData 0.002 (0.004)\tLoss 0.3733 (0.4495)\tPrec 85.156% (84.058%)\n",
      "Epoch: [20][200/391]\tTime 0.081 (0.083)\tData 0.002 (0.003)\tLoss 0.4306 (0.4531)\tPrec 83.594% (84.029%)\n",
      "Epoch: [20][300/391]\tTime 0.089 (0.083)\tData 0.003 (0.003)\tLoss 0.5581 (0.4533)\tPrec 76.562% (84.147%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.245 (0.245)\tLoss 0.5230 (0.5230)\tPrec 83.594% (83.594%)\n",
      " * Prec 81.800% \n",
      "best acc: 81.800000\n",
      "Epoch: [21][0/391]\tTime 0.301 (0.301)\tData 0.244 (0.244)\tLoss 0.4698 (0.4698)\tPrec 84.375% (84.375%)\n",
      "Epoch: [21][100/391]\tTime 0.069 (0.077)\tData 0.002 (0.005)\tLoss 0.4655 (0.4431)\tPrec 84.375% (84.398%)\n",
      "Epoch: [21][200/391]\tTime 0.088 (0.079)\tData 0.002 (0.003)\tLoss 0.5443 (0.4487)\tPrec 81.250% (84.356%)\n",
      "Epoch: [21][300/391]\tTime 0.087 (0.080)\tData 0.002 (0.003)\tLoss 0.3210 (0.4457)\tPrec 89.844% (84.526%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.236 (0.236)\tLoss 0.4731 (0.4731)\tPrec 83.594% (83.594%)\n",
      " * Prec 81.010% \n",
      "best acc: 81.800000\n",
      "Epoch: [22][0/391]\tTime 0.284 (0.284)\tData 0.214 (0.214)\tLoss 0.3375 (0.3375)\tPrec 90.625% (90.625%)\n",
      "Epoch: [22][100/391]\tTime 0.079 (0.074)\tData 0.003 (0.004)\tLoss 0.5526 (0.4218)\tPrec 81.250% (85.644%)\n",
      "Epoch: [22][200/391]\tTime 0.080 (0.079)\tData 0.002 (0.003)\tLoss 0.3887 (0.4302)\tPrec 87.500% (85.180%)\n",
      "Epoch: [22][300/391]\tTime 0.090 (0.081)\tData 0.003 (0.003)\tLoss 0.5211 (0.4334)\tPrec 84.375% (84.951%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.256 (0.256)\tLoss 0.4567 (0.4567)\tPrec 85.156% (85.156%)\n",
      " * Prec 80.740% \n",
      "best acc: 81.800000\n",
      "Epoch: [23][0/391]\tTime 0.282 (0.282)\tData 0.216 (0.216)\tLoss 0.4130 (0.4130)\tPrec 87.500% (87.500%)\n",
      "Epoch: [23][100/391]\tTime 0.065 (0.080)\tData 0.003 (0.005)\tLoss 0.5486 (0.4170)\tPrec 81.250% (85.412%)\n",
      "Epoch: [23][200/391]\tTime 0.086 (0.080)\tData 0.002 (0.004)\tLoss 0.3591 (0.4202)\tPrec 87.500% (85.187%)\n",
      "Epoch: [23][300/391]\tTime 0.085 (0.081)\tData 0.003 (0.003)\tLoss 0.3548 (0.4268)\tPrec 88.281% (85.024%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.243 (0.243)\tLoss 0.7945 (0.7945)\tPrec 73.438% (73.438%)\n",
      " * Prec 76.800% \n",
      "best acc: 81.800000\n",
      "Epoch: [24][0/391]\tTime 0.335 (0.335)\tData 0.253 (0.253)\tLoss 0.4375 (0.4375)\tPrec 83.594% (83.594%)\n",
      "Epoch: [24][100/391]\tTime 0.060 (0.084)\tData 0.002 (0.005)\tLoss 0.4188 (0.4170)\tPrec 85.938% (85.450%)\n",
      "Epoch: [24][200/391]\tTime 0.084 (0.080)\tData 0.002 (0.003)\tLoss 0.3067 (0.4225)\tPrec 91.406% (85.316%)\n",
      "Epoch: [24][300/391]\tTime 0.083 (0.081)\tData 0.002 (0.003)\tLoss 0.3692 (0.4204)\tPrec 89.062% (85.379%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.242 (0.242)\tLoss 0.6994 (0.6994)\tPrec 82.812% (82.812%)\n",
      " * Prec 75.700% \n",
      "best acc: 81.800000\n",
      "Epoch: [25][0/391]\tTime 0.305 (0.305)\tData 0.236 (0.236)\tLoss 0.3032 (0.3032)\tPrec 90.625% (90.625%)\n",
      "Epoch: [25][100/391]\tTime 0.080 (0.086)\tData 0.002 (0.005)\tLoss 0.4281 (0.3951)\tPrec 86.719% (86.208%)\n",
      "Epoch: [25][200/391]\tTime 0.088 (0.080)\tData 0.002 (0.003)\tLoss 0.4644 (0.4074)\tPrec 82.812% (85.755%)\n",
      "Epoch: [25][300/391]\tTime 0.081 (0.081)\tData 0.003 (0.003)\tLoss 0.5052 (0.4111)\tPrec 82.031% (85.582%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.244 (0.244)\tLoss 0.5836 (0.5836)\tPrec 82.812% (82.812%)\n",
      " * Prec 80.150% \n",
      "best acc: 81.800000\n",
      "Epoch: [26][0/391]\tTime 0.306 (0.306)\tData 0.237 (0.237)\tLoss 0.2983 (0.2983)\tPrec 88.281% (88.281%)\n",
      "Epoch: [26][100/391]\tTime 0.087 (0.086)\tData 0.002 (0.004)\tLoss 0.5971 (0.4108)\tPrec 78.906% (85.814%)\n",
      "Epoch: [26][200/391]\tTime 0.057 (0.081)\tData 0.002 (0.003)\tLoss 0.4682 (0.4134)\tPrec 84.375% (85.627%)\n",
      "Epoch: [26][300/391]\tTime 0.084 (0.080)\tData 0.002 (0.003)\tLoss 0.4508 (0.4122)\tPrec 84.375% (85.613%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.234 (0.234)\tLoss 0.5036 (0.5036)\tPrec 83.594% (83.594%)\n",
      " * Prec 81.190% \n",
      "best acc: 81.800000\n",
      "Epoch: [27][0/391]\tTime 0.311 (0.311)\tData 0.220 (0.220)\tLoss 0.2349 (0.2349)\tPrec 91.406% (91.406%)\n",
      "Epoch: [27][100/391]\tTime 0.084 (0.087)\tData 0.003 (0.005)\tLoss 0.6026 (0.3955)\tPrec 79.688% (86.247%)\n",
      "Epoch: [27][200/391]\tTime 0.068 (0.084)\tData 0.003 (0.003)\tLoss 0.3877 (0.3917)\tPrec 86.719% (86.427%)\n",
      "Epoch: [27][300/391]\tTime 0.084 (0.082)\tData 0.002 (0.003)\tLoss 0.6725 (0.3961)\tPrec 76.562% (86.194%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.250 (0.250)\tLoss 0.5556 (0.5556)\tPrec 80.469% (80.469%)\n",
      " * Prec 80.150% \n",
      "best acc: 81.800000\n",
      "Epoch: [28][0/391]\tTime 0.269 (0.269)\tData 0.209 (0.209)\tLoss 0.4420 (0.4420)\tPrec 83.594% (83.594%)\n",
      "Epoch: [28][100/391]\tTime 0.087 (0.085)\tData 0.002 (0.004)\tLoss 0.3494 (0.3744)\tPrec 86.719% (87.307%)\n",
      "Epoch: [28][200/391]\tTime 0.083 (0.085)\tData 0.002 (0.003)\tLoss 0.4169 (0.3863)\tPrec 86.719% (86.866%)\n",
      "Epoch: [28][300/391]\tTime 0.078 (0.081)\tData 0.002 (0.003)\tLoss 0.4370 (0.3928)\tPrec 89.062% (86.498%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.256 (0.256)\tLoss 0.5357 (0.5357)\tPrec 81.250% (81.250%)\n",
      " * Prec 73.820% \n",
      "best acc: 81.800000\n",
      "Epoch: [29][0/391]\tTime 0.309 (0.309)\tData 0.217 (0.217)\tLoss 0.4135 (0.4135)\tPrec 85.156% (85.156%)\n",
      "Epoch: [29][100/391]\tTime 0.087 (0.086)\tData 0.002 (0.004)\tLoss 0.4034 (0.3927)\tPrec 85.938% (86.580%)\n",
      "Epoch: [29][200/391]\tTime 0.084 (0.085)\tData 0.002 (0.003)\tLoss 0.4569 (0.3850)\tPrec 83.594% (86.528%)\n",
      "Epoch: [29][300/391]\tTime 0.073 (0.083)\tData 0.003 (0.003)\tLoss 0.2927 (0.3857)\tPrec 89.062% (86.418%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.275 (0.275)\tLoss 0.5058 (0.5058)\tPrec 82.031% (82.031%)\n",
      " * Prec 81.170% \n",
      "best acc: 81.800000\n",
      "Epoch: [30][0/391]\tTime 0.296 (0.296)\tData 0.219 (0.219)\tLoss 0.4036 (0.4036)\tPrec 88.281% (88.281%)\n",
      "Epoch: [30][100/391]\tTime 0.078 (0.087)\tData 0.002 (0.004)\tLoss 0.3440 (0.3830)\tPrec 88.281% (86.680%)\n",
      "Epoch: [30][200/391]\tTime 0.086 (0.085)\tData 0.002 (0.003)\tLoss 0.3996 (0.3857)\tPrec 85.938% (86.451%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][300/391]\tTime 0.067 (0.083)\tData 0.002 (0.003)\tLoss 0.3349 (0.3853)\tPrec 89.062% (86.477%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.238 (0.238)\tLoss 0.4027 (0.4027)\tPrec 84.375% (84.375%)\n",
      " * Prec 82.760% \n",
      "best acc: 82.760000\n",
      "Epoch: [31][0/391]\tTime 0.293 (0.293)\tData 0.221 (0.221)\tLoss 0.5037 (0.5037)\tPrec 83.594% (83.594%)\n",
      "Epoch: [31][100/391]\tTime 0.087 (0.082)\tData 0.002 (0.004)\tLoss 0.5843 (0.3700)\tPrec 78.125% (86.935%)\n",
      "Epoch: [31][200/391]\tTime 0.081 (0.083)\tData 0.002 (0.003)\tLoss 0.4982 (0.3711)\tPrec 85.156% (86.866%)\n",
      "Epoch: [31][300/391]\tTime 0.083 (0.083)\tData 0.002 (0.003)\tLoss 0.3629 (0.3705)\tPrec 85.938% (86.846%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.226 (0.226)\tLoss 0.5611 (0.5611)\tPrec 80.469% (80.469%)\n",
      " * Prec 79.710% \n",
      "best acc: 82.760000\n",
      "Epoch: [32][0/391]\tTime 0.295 (0.295)\tData 0.236 (0.236)\tLoss 0.1901 (0.1901)\tPrec 93.750% (93.750%)\n",
      "Epoch: [32][100/391]\tTime 0.086 (0.084)\tData 0.002 (0.004)\tLoss 0.2912 (0.3626)\tPrec 90.625% (87.384%)\n",
      "Epoch: [32][200/391]\tTime 0.077 (0.084)\tData 0.003 (0.003)\tLoss 0.3813 (0.3641)\tPrec 85.156% (87.352%)\n",
      "Epoch: [32][300/391]\tTime 0.085 (0.084)\tData 0.002 (0.003)\tLoss 0.4617 (0.3697)\tPrec 82.812% (87.139%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.267 (0.267)\tLoss 0.4942 (0.4942)\tPrec 85.938% (85.938%)\n",
      " * Prec 83.270% \n",
      "best acc: 83.270000\n",
      "Epoch: [33][0/391]\tTime 0.324 (0.324)\tData 0.249 (0.249)\tLoss 0.4222 (0.4222)\tPrec 86.719% (86.719%)\n",
      "Epoch: [33][100/391]\tTime 0.088 (0.087)\tData 0.003 (0.005)\tLoss 0.3240 (0.3566)\tPrec 85.938% (87.407%)\n",
      "Epoch: [33][200/391]\tTime 0.088 (0.085)\tData 0.002 (0.004)\tLoss 0.2659 (0.3541)\tPrec 89.844% (87.442%)\n",
      "Epoch: [33][300/391]\tTime 0.076 (0.084)\tData 0.002 (0.003)\tLoss 0.4976 (0.3597)\tPrec 82.812% (87.266%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.261 (0.261)\tLoss 0.5650 (0.5650)\tPrec 82.031% (82.031%)\n",
      " * Prec 82.240% \n",
      "best acc: 83.270000\n",
      "Epoch: [34][0/391]\tTime 0.400 (0.400)\tData 0.341 (0.341)\tLoss 0.4153 (0.4153)\tPrec 87.500% (87.500%)\n",
      "Epoch: [34][100/391]\tTime 0.077 (0.085)\tData 0.002 (0.006)\tLoss 0.3495 (0.3479)\tPrec 86.719% (87.848%)\n",
      "Epoch: [34][200/391]\tTime 0.084 (0.085)\tData 0.002 (0.004)\tLoss 0.5167 (0.3582)\tPrec 84.375% (87.508%)\n",
      "Epoch: [34][300/391]\tTime 0.088 (0.084)\tData 0.002 (0.003)\tLoss 0.4271 (0.3632)\tPrec 85.938% (87.274%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.213 (0.213)\tLoss 1.0445 (1.0445)\tPrec 71.875% (71.875%)\n",
      " * Prec 70.260% \n",
      "best acc: 83.270000\n",
      "Epoch: [35][0/391]\tTime 0.316 (0.316)\tData 0.255 (0.255)\tLoss 0.4600 (0.4600)\tPrec 84.375% (84.375%)\n",
      "Epoch: [35][100/391]\tTime 0.082 (0.081)\tData 0.002 (0.005)\tLoss 0.4060 (0.3505)\tPrec 82.812% (87.678%)\n",
      "Epoch: [35][200/391]\tTime 0.086 (0.083)\tData 0.002 (0.003)\tLoss 0.4555 (0.3563)\tPrec 86.719% (87.593%)\n",
      "Epoch: [35][300/391]\tTime 0.085 (0.082)\tData 0.002 (0.003)\tLoss 0.4048 (0.3558)\tPrec 81.250% (87.477%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.284 (0.284)\tLoss 0.4671 (0.4671)\tPrec 82.812% (82.812%)\n",
      " * Prec 84.320% \n",
      "best acc: 84.320000\n",
      "Epoch: [36][0/391]\tTime 0.335 (0.335)\tData 0.255 (0.255)\tLoss 0.3250 (0.3250)\tPrec 88.281% (88.281%)\n",
      "Epoch: [36][100/391]\tTime 0.086 (0.078)\tData 0.003 (0.005)\tLoss 0.3311 (0.3406)\tPrec 87.500% (88.041%)\n",
      "Epoch: [36][200/391]\tTime 0.086 (0.081)\tData 0.002 (0.004)\tLoss 0.2993 (0.3474)\tPrec 90.625% (87.846%)\n",
      "Epoch: [36][300/391]\tTime 0.084 (0.082)\tData 0.002 (0.003)\tLoss 0.3962 (0.3503)\tPrec 87.500% (87.770%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.273 (0.273)\tLoss 0.5062 (0.5062)\tPrec 79.688% (79.688%)\n",
      " * Prec 81.660% \n",
      "best acc: 84.320000\n",
      "Epoch: [37][0/391]\tTime 0.303 (0.303)\tData 0.242 (0.242)\tLoss 0.2766 (0.2766)\tPrec 93.750% (93.750%)\n",
      "Epoch: [37][100/391]\tTime 0.085 (0.079)\tData 0.002 (0.005)\tLoss 0.4107 (0.3453)\tPrec 82.812% (88.003%)\n",
      "Epoch: [37][200/391]\tTime 0.083 (0.081)\tData 0.002 (0.003)\tLoss 0.2001 (0.3433)\tPrec 91.406% (88.083%)\n",
      "Epoch: [37][300/391]\tTime 0.082 (0.082)\tData 0.002 (0.003)\tLoss 0.2655 (0.3421)\tPrec 90.625% (88.133%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.244 (0.244)\tLoss 0.3638 (0.3638)\tPrec 88.281% (88.281%)\n",
      " * Prec 84.630% \n",
      "best acc: 84.630000\n",
      "Epoch: [38][0/391]\tTime 0.324 (0.324)\tData 0.259 (0.259)\tLoss 0.3965 (0.3965)\tPrec 87.500% (87.500%)\n",
      "Epoch: [38][100/391]\tTime 0.072 (0.083)\tData 0.002 (0.005)\tLoss 0.3310 (0.3392)\tPrec 87.500% (88.204%)\n",
      "Epoch: [38][200/391]\tTime 0.089 (0.081)\tData 0.002 (0.004)\tLoss 0.3244 (0.3387)\tPrec 86.719% (88.258%)\n",
      "Epoch: [38][300/391]\tTime 0.084 (0.082)\tData 0.002 (0.003)\tLoss 0.4189 (0.3428)\tPrec 84.375% (88.042%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.278 (0.278)\tLoss 0.4470 (0.4470)\tPrec 84.375% (84.375%)\n",
      " * Prec 82.940% \n",
      "best acc: 84.630000\n",
      "Epoch: [39][0/391]\tTime 0.300 (0.300)\tData 0.226 (0.226)\tLoss 0.2153 (0.2153)\tPrec 92.969% (92.969%)\n",
      "Epoch: [39][100/391]\tTime 0.082 (0.086)\tData 0.002 (0.005)\tLoss 0.3047 (0.3206)\tPrec 87.500% (88.815%)\n",
      "Epoch: [39][200/391]\tTime 0.087 (0.079)\tData 0.003 (0.003)\tLoss 0.2800 (0.3295)\tPrec 90.625% (88.511%)\n",
      "Epoch: [39][300/391]\tTime 0.087 (0.080)\tData 0.002 (0.003)\tLoss 0.2743 (0.3338)\tPrec 91.406% (88.338%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.270 (0.270)\tLoss 0.5085 (0.5085)\tPrec 82.031% (82.031%)\n",
      " * Prec 83.030% \n",
      "best acc: 84.630000\n",
      "Epoch: [40][0/391]\tTime 0.285 (0.285)\tData 0.224 (0.224)\tLoss 0.2995 (0.2995)\tPrec 89.062% (89.062%)\n",
      "Epoch: [40][100/391]\tTime 0.080 (0.086)\tData 0.002 (0.004)\tLoss 0.2735 (0.3231)\tPrec 90.625% (88.707%)\n",
      "Epoch: [40][200/391]\tTime 0.075 (0.079)\tData 0.003 (0.003)\tLoss 0.3967 (0.3274)\tPrec 86.719% (88.573%)\n",
      "Epoch: [40][300/391]\tTime 0.089 (0.080)\tData 0.002 (0.003)\tLoss 0.3255 (0.3283)\tPrec 92.188% (88.491%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.251 (0.251)\tLoss 0.5789 (0.5789)\tPrec 83.594% (83.594%)\n",
      " * Prec 81.770% \n",
      "best acc: 84.630000\n",
      "Epoch: [41][0/391]\tTime 0.254 (0.254)\tData 0.209 (0.209)\tLoss 0.1772 (0.1772)\tPrec 94.531% (94.531%)\n",
      "Epoch: [41][100/391]\tTime 0.081 (0.084)\tData 0.002 (0.004)\tLoss 0.2944 (0.3169)\tPrec 89.062% (88.815%)\n",
      "Epoch: [41][200/391]\tTime 0.067 (0.081)\tData 0.003 (0.003)\tLoss 0.3191 (0.3241)\tPrec 88.281% (88.705%)\n",
      "Epoch: [41][300/391]\tTime 0.084 (0.080)\tData 0.002 (0.003)\tLoss 0.4091 (0.3260)\tPrec 86.719% (88.715%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.258 (0.258)\tLoss 0.4385 (0.4385)\tPrec 82.031% (82.031%)\n",
      " * Prec 83.670% \n",
      "best acc: 84.630000\n",
      "Epoch: [42][0/391]\tTime 0.297 (0.297)\tData 0.228 (0.228)\tLoss 0.2799 (0.2799)\tPrec 90.625% (90.625%)\n",
      "Epoch: [42][100/391]\tTime 0.085 (0.086)\tData 0.002 (0.004)\tLoss 0.3704 (0.3098)\tPrec 86.719% (89.418%)\n",
      "Epoch: [42][200/391]\tTime 0.063 (0.084)\tData 0.002 (0.003)\tLoss 0.3138 (0.3194)\tPrec 91.406% (89.004%)\n",
      "Epoch: [42][300/391]\tTime 0.088 (0.080)\tData 0.002 (0.003)\tLoss 0.3484 (0.3226)\tPrec 88.281% (88.795%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.245 (0.245)\tLoss 0.3812 (0.3812)\tPrec 85.156% (85.156%)\n",
      " * Prec 83.480% \n",
      "best acc: 84.630000\n",
      "Epoch: [43][0/391]\tTime 0.275 (0.275)\tData 0.211 (0.211)\tLoss 0.2915 (0.2915)\tPrec 90.625% (90.625%)\n",
      "Epoch: [43][100/391]\tTime 0.087 (0.086)\tData 0.002 (0.004)\tLoss 0.3074 (0.3115)\tPrec 90.625% (89.055%)\n",
      "Epoch: [43][200/391]\tTime 0.087 (0.085)\tData 0.002 (0.003)\tLoss 0.2830 (0.3202)\tPrec 91.406% (88.822%)\n",
      "Epoch: [43][300/391]\tTime 0.079 (0.081)\tData 0.003 (0.003)\tLoss 0.2966 (0.3220)\tPrec 92.188% (88.780%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.271 (0.271)\tLoss 0.3770 (0.3770)\tPrec 85.156% (85.156%)\n",
      " * Prec 84.790% \n",
      "best acc: 84.790000\n",
      "Epoch: [44][0/391]\tTime 0.273 (0.273)\tData 0.216 (0.216)\tLoss 0.2260 (0.2260)\tPrec 91.406% (91.406%)\n",
      "Epoch: [44][100/391]\tTime 0.080 (0.085)\tData 0.002 (0.004)\tLoss 0.3720 (0.3168)\tPrec 85.156% (88.761%)\n",
      "Epoch: [44][200/391]\tTime 0.077 (0.084)\tData 0.002 (0.003)\tLoss 0.2529 (0.3185)\tPrec 92.188% (88.888%)\n",
      "Epoch: [44][300/391]\tTime 0.064 (0.082)\tData 0.002 (0.003)\tLoss 0.3243 (0.3194)\tPrec 91.406% (88.800%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.242 (0.242)\tLoss 0.4695 (0.4695)\tPrec 83.594% (83.594%)\n",
      " * Prec 84.350% \n",
      "best acc: 84.790000\n",
      "Epoch: [45][0/391]\tTime 0.301 (0.301)\tData 0.235 (0.235)\tLoss 0.2358 (0.2358)\tPrec 91.406% (91.406%)\n",
      "Epoch: [45][100/391]\tTime 0.096 (0.086)\tData 0.002 (0.005)\tLoss 0.2064 (0.3057)\tPrec 92.188% (89.163%)\n",
      "Epoch: [45][200/391]\tTime 0.088 (0.085)\tData 0.002 (0.004)\tLoss 0.3336 (0.3075)\tPrec 89.062% (89.070%)\n",
      "Epoch: [45][300/391]\tTime 0.076 (0.084)\tData 0.002 (0.003)\tLoss 0.3613 (0.3135)\tPrec 87.500% (88.959%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation starts\n",
      "Test: [0/79]\tTime 0.270 (0.270)\tLoss 0.4785 (0.4785)\tPrec 85.156% (85.156%)\n",
      " * Prec 84.020% \n",
      "best acc: 84.790000\n",
      "Epoch: [46][0/391]\tTime 0.313 (0.313)\tData 0.242 (0.242)\tLoss 0.3147 (0.3147)\tPrec 86.719% (86.719%)\n",
      "Epoch: [46][100/391]\tTime 0.087 (0.083)\tData 0.002 (0.005)\tLoss 0.4413 (0.2986)\tPrec 86.719% (89.341%)\n",
      "Epoch: [46][200/391]\tTime 0.075 (0.084)\tData 0.002 (0.003)\tLoss 0.3383 (0.3008)\tPrec 85.156% (89.296%)\n",
      "Epoch: [46][300/391]\tTime 0.084 (0.083)\tData 0.002 (0.003)\tLoss 0.1763 (0.3008)\tPrec 95.312% (89.351%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.266 (0.266)\tLoss 0.6499 (0.6499)\tPrec 83.594% (83.594%)\n",
      " * Prec 81.210% \n",
      "best acc: 84.790000\n",
      "Epoch: [47][0/391]\tTime 0.329 (0.329)\tData 0.267 (0.267)\tLoss 0.3335 (0.3335)\tPrec 89.062% (89.062%)\n",
      "Epoch: [47][100/391]\tTime 0.082 (0.085)\tData 0.002 (0.005)\tLoss 0.1989 (0.3068)\tPrec 92.969% (89.496%)\n",
      "Epoch: [47][200/391]\tTime 0.085 (0.085)\tData 0.002 (0.004)\tLoss 0.1721 (0.3096)\tPrec 92.969% (89.373%)\n",
      "Epoch: [47][300/391]\tTime 0.079 (0.085)\tData 0.002 (0.003)\tLoss 0.2434 (0.3093)\tPrec 90.625% (89.283%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.212 (0.212)\tLoss 0.3769 (0.3769)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.430% \n",
      "best acc: 85.430000\n",
      "Epoch: [48][0/391]\tTime 0.442 (0.442)\tData 0.370 (0.370)\tLoss 0.3647 (0.3647)\tPrec 84.375% (84.375%)\n",
      "Epoch: [48][100/391]\tTime 0.086 (0.087)\tData 0.002 (0.006)\tLoss 0.4806 (0.2874)\tPrec 86.719% (89.844%)\n",
      "Epoch: [48][200/391]\tTime 0.087 (0.086)\tData 0.002 (0.004)\tLoss 0.2699 (0.2952)\tPrec 89.062% (89.587%)\n",
      "Epoch: [48][300/391]\tTime 0.086 (0.085)\tData 0.002 (0.004)\tLoss 0.3932 (0.2999)\tPrec 82.031% (89.441%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.243 (0.243)\tLoss 0.4051 (0.4051)\tPrec 86.719% (86.719%)\n",
      " * Prec 86.230% \n",
      "best acc: 86.230000\n",
      "Epoch: [49][0/391]\tTime 0.326 (0.326)\tData 0.262 (0.262)\tLoss 0.1931 (0.1931)\tPrec 94.531% (94.531%)\n",
      "Epoch: [49][100/391]\tTime 0.084 (0.083)\tData 0.003 (0.005)\tLoss 0.2698 (0.2870)\tPrec 88.281% (89.828%)\n",
      "Epoch: [49][200/391]\tTime 0.081 (0.083)\tData 0.002 (0.004)\tLoss 0.3589 (0.2961)\tPrec 85.938% (89.471%)\n",
      "Epoch: [49][300/391]\tTime 0.090 (0.083)\tData 0.007 (0.003)\tLoss 0.3076 (0.2997)\tPrec 90.625% (89.332%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.239 (0.239)\tLoss 0.4431 (0.4431)\tPrec 84.375% (84.375%)\n",
      " * Prec 84.440% \n",
      "best acc: 86.230000\n",
      "Epoch: [50][0/391]\tTime 0.318 (0.318)\tData 0.247 (0.247)\tLoss 0.2780 (0.2780)\tPrec 91.406% (91.406%)\n",
      "Epoch: [50][100/391]\tTime 0.086 (0.078)\tData 0.002 (0.005)\tLoss 0.2287 (0.2814)\tPrec 91.406% (90.176%)\n",
      "Epoch: [50][200/391]\tTime 0.080 (0.081)\tData 0.002 (0.003)\tLoss 0.3691 (0.2933)\tPrec 87.500% (89.704%)\n",
      "Epoch: [50][300/391]\tTime 0.088 (0.082)\tData 0.002 (0.003)\tLoss 0.2891 (0.2960)\tPrec 90.625% (89.631%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.361 (0.361)\tLoss 0.4270 (0.4270)\tPrec 85.156% (85.156%)\n",
      " * Prec 85.110% \n",
      "best acc: 86.230000\n",
      "Epoch: [51][0/391]\tTime 0.285 (0.285)\tData 0.227 (0.227)\tLoss 0.2173 (0.2173)\tPrec 91.406% (91.406%)\n",
      "Epoch: [51][100/391]\tTime 0.081 (0.076)\tData 0.002 (0.004)\tLoss 0.3600 (0.2825)\tPrec 89.062% (90.200%)\n",
      "Epoch: [51][200/391]\tTime 0.082 (0.080)\tData 0.002 (0.003)\tLoss 0.3186 (0.2915)\tPrec 88.281% (89.809%)\n",
      "Epoch: [51][300/391]\tTime 0.090 (0.081)\tData 0.002 (0.003)\tLoss 0.2724 (0.2910)\tPrec 89.062% (89.833%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.294 (0.294)\tLoss 0.4252 (0.4252)\tPrec 86.719% (86.719%)\n",
      " * Prec 82.710% \n",
      "best acc: 86.230000\n",
      "Epoch: [52][0/391]\tTime 0.299 (0.299)\tData 0.230 (0.230)\tLoss 0.3559 (0.3559)\tPrec 88.281% (88.281%)\n",
      "Epoch: [52][100/391]\tTime 0.056 (0.079)\tData 0.002 (0.004)\tLoss 0.2912 (0.2875)\tPrec 89.844% (89.751%)\n",
      "Epoch: [52][200/391]\tTime 0.086 (0.081)\tData 0.002 (0.003)\tLoss 0.2062 (0.2920)\tPrec 92.969% (89.669%)\n",
      "Epoch: [52][300/391]\tTime 0.079 (0.082)\tData 0.002 (0.003)\tLoss 0.2819 (0.2984)\tPrec 89.844% (89.460%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.248 (0.248)\tLoss 0.3644 (0.3644)\tPrec 88.281% (88.281%)\n",
      " * Prec 83.460% \n",
      "best acc: 86.230000\n",
      "Epoch: [53][0/391]\tTime 0.304 (0.304)\tData 0.258 (0.258)\tLoss 0.3023 (0.3023)\tPrec 86.719% (86.719%)\n",
      "Epoch: [53][100/391]\tTime 0.073 (0.083)\tData 0.002 (0.005)\tLoss 0.4163 (0.2862)\tPrec 85.938% (90.076%)\n",
      "Epoch: [53][200/391]\tTime 0.085 (0.082)\tData 0.003 (0.004)\tLoss 0.2549 (0.2902)\tPrec 90.625% (90.007%)\n",
      "Epoch: [53][300/391]\tTime 0.084 (0.083)\tData 0.002 (0.003)\tLoss 0.2658 (0.2898)\tPrec 90.625% (90.015%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.235 (0.235)\tLoss 0.4432 (0.4432)\tPrec 85.938% (85.938%)\n",
      " * Prec 84.710% \n",
      "best acc: 86.230000\n",
      "Epoch: [54][0/391]\tTime 0.319 (0.319)\tData 0.250 (0.250)\tLoss 0.2639 (0.2639)\tPrec 88.281% (88.281%)\n",
      "Epoch: [54][100/391]\tTime 0.079 (0.085)\tData 0.002 (0.005)\tLoss 0.2277 (0.2836)\tPrec 91.406% (89.960%)\n",
      "Epoch: [54][200/391]\tTime 0.088 (0.080)\tData 0.002 (0.004)\tLoss 0.2649 (0.2852)\tPrec 92.188% (89.941%)\n",
      "Epoch: [54][300/391]\tTime 0.084 (0.081)\tData 0.002 (0.003)\tLoss 0.2710 (0.2878)\tPrec 89.062% (89.768%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.251 (0.251)\tLoss 0.3952 (0.3952)\tPrec 86.719% (86.719%)\n",
      " * Prec 83.710% \n",
      "best acc: 86.230000\n",
      "Epoch: [55][0/391]\tTime 0.290 (0.290)\tData 0.231 (0.231)\tLoss 0.3616 (0.3616)\tPrec 88.281% (88.281%)\n",
      "Epoch: [55][100/391]\tTime 0.083 (0.085)\tData 0.002 (0.005)\tLoss 0.3342 (0.2835)\tPrec 87.500% (90.037%)\n",
      "Epoch: [55][200/391]\tTime 0.077 (0.080)\tData 0.002 (0.003)\tLoss 0.2078 (0.2823)\tPrec 92.188% (90.124%)\n",
      "Epoch: [55][300/391]\tTime 0.082 (0.079)\tData 0.002 (0.003)\tLoss 0.3596 (0.2834)\tPrec 85.938% (90.062%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.244 (0.244)\tLoss 0.4605 (0.4605)\tPrec 85.938% (85.938%)\n",
      " * Prec 84.150% \n",
      "best acc: 86.230000\n",
      "Epoch: [56][0/391]\tTime 0.252 (0.252)\tData 0.206 (0.206)\tLoss 0.3041 (0.3041)\tPrec 86.719% (86.719%)\n",
      "Epoch: [56][100/391]\tTime 0.085 (0.085)\tData 0.002 (0.004)\tLoss 0.2140 (0.2646)\tPrec 92.969% (90.640%)\n",
      "Epoch: [56][200/391]\tTime 0.071 (0.083)\tData 0.003 (0.003)\tLoss 0.2245 (0.2752)\tPrec 92.188% (90.415%)\n",
      "Epoch: [56][300/391]\tTime 0.084 (0.078)\tData 0.002 (0.003)\tLoss 0.1864 (0.2767)\tPrec 92.188% (90.355%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.226 (0.226)\tLoss 0.4166 (0.4166)\tPrec 86.719% (86.719%)\n",
      " * Prec 84.830% \n",
      "best acc: 86.230000\n",
      "Epoch: [57][0/391]\tTime 0.316 (0.316)\tData 0.246 (0.246)\tLoss 0.3250 (0.3250)\tPrec 85.938% (85.938%)\n",
      "Epoch: [57][100/391]\tTime 0.081 (0.082)\tData 0.002 (0.004)\tLoss 0.3526 (0.2635)\tPrec 86.719% (90.586%)\n",
      "Epoch: [57][200/391]\tTime 0.075 (0.082)\tData 0.002 (0.003)\tLoss 0.2542 (0.2778)\tPrec 92.188% (90.139%)\n",
      "Epoch: [57][300/391]\tTime 0.044 (0.078)\tData 0.002 (0.003)\tLoss 0.2703 (0.2816)\tPrec 90.625% (90.025%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.239 (0.239)\tLoss 0.4739 (0.4739)\tPrec 85.156% (85.156%)\n",
      " * Prec 83.440% \n",
      "best acc: 86.230000\n",
      "Epoch: [58][0/391]\tTime 0.287 (0.287)\tData 0.230 (0.230)\tLoss 0.2350 (0.2350)\tPrec 93.750% (93.750%)\n",
      "Epoch: [58][100/391]\tTime 0.083 (0.081)\tData 0.002 (0.004)\tLoss 0.2436 (0.2626)\tPrec 90.625% (90.857%)\n",
      "Epoch: [58][200/391]\tTime 0.087 (0.082)\tData 0.002 (0.003)\tLoss 0.1949 (0.2783)\tPrec 92.969% (90.194%)\n",
      "Epoch: [58][300/391]\tTime 0.068 (0.082)\tData 0.003 (0.003)\tLoss 0.2377 (0.2791)\tPrec 93.750% (90.207%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.217 (0.217)\tLoss 0.4607 (0.4607)\tPrec 84.375% (84.375%)\n",
      " * Prec 85.850% \n",
      "best acc: 86.230000\n",
      "Epoch: [59][0/391]\tTime 0.297 (0.297)\tData 0.221 (0.221)\tLoss 0.3946 (0.3946)\tPrec 85.938% (85.938%)\n",
      "Epoch: [59][100/391]\tTime 0.080 (0.083)\tData 0.002 (0.004)\tLoss 0.3569 (0.2656)\tPrec 88.281% (91.004%)\n",
      "Epoch: [59][200/391]\tTime 0.086 (0.083)\tData 0.003 (0.003)\tLoss 0.2987 (0.2738)\tPrec 90.625% (90.625%)\n",
      "Epoch: [59][300/391]\tTime 0.069 (0.083)\tData 0.002 (0.003)\tLoss 0.4350 (0.2760)\tPrec 82.031% (90.433%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.245 (0.245)\tLoss 0.3837 (0.3837)\tPrec 89.062% (89.062%)\n",
      " * Prec 85.100% \n",
      "best acc: 86.230000\n",
      "Epoch: [60][0/391]\tTime 0.289 (0.289)\tData 0.217 (0.217)\tLoss 0.2562 (0.2562)\tPrec 89.844% (89.844%)\n",
      "Epoch: [60][100/391]\tTime 0.078 (0.085)\tData 0.003 (0.004)\tLoss 0.3386 (0.2650)\tPrec 91.406% (90.455%)\n",
      "Epoch: [60][200/391]\tTime 0.078 (0.081)\tData 0.002 (0.003)\tLoss 0.2440 (0.2756)\tPrec 92.188% (90.279%)\n",
      "Epoch: [60][300/391]\tTime 0.080 (0.081)\tData 0.002 (0.003)\tLoss 0.3002 (0.2759)\tPrec 92.969% (90.230%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.249 (0.249)\tLoss 0.4531 (0.4531)\tPrec 84.375% (84.375%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Prec 84.110% \n",
      "best acc: 86.230000\n",
      "Epoch: [61][0/391]\tTime 0.272 (0.272)\tData 0.206 (0.206)\tLoss 0.2817 (0.2817)\tPrec 89.062% (89.062%)\n",
      "Epoch: [61][100/391]\tTime 0.085 (0.073)\tData 0.002 (0.004)\tLoss 0.2244 (0.2703)\tPrec 92.188% (90.370%)\n",
      "Epoch: [61][200/391]\tTime 0.082 (0.078)\tData 0.002 (0.003)\tLoss 0.2315 (0.2632)\tPrec 89.062% (90.652%)\n",
      "Epoch: [61][300/391]\tTime 0.079 (0.079)\tData 0.002 (0.003)\tLoss 0.1745 (0.2689)\tPrec 92.969% (90.404%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.231 (0.231)\tLoss 0.3331 (0.3331)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.870% \n",
      "best acc: 86.230000\n",
      "Epoch: [62][0/391]\tTime 0.294 (0.294)\tData 0.235 (0.235)\tLoss 0.3386 (0.3386)\tPrec 87.500% (87.500%)\n",
      "Epoch: [62][100/391]\tTime 0.081 (0.073)\tData 0.003 (0.004)\tLoss 0.2238 (0.2603)\tPrec 92.188% (90.888%)\n",
      "Epoch: [62][200/391]\tTime 0.085 (0.078)\tData 0.002 (0.003)\tLoss 0.3201 (0.2649)\tPrec 89.844% (90.699%)\n",
      "Epoch: [62][300/391]\tTime 0.087 (0.080)\tData 0.003 (0.003)\tLoss 0.2967 (0.2672)\tPrec 88.281% (90.586%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.281 (0.281)\tLoss 0.4831 (0.4831)\tPrec 86.719% (86.719%)\n",
      " * Prec 84.170% \n",
      "best acc: 86.230000\n",
      "Epoch: [63][0/391]\tTime 0.323 (0.323)\tData 0.255 (0.255)\tLoss 0.3963 (0.3963)\tPrec 86.719% (86.719%)\n",
      "Epoch: [63][100/391]\tTime 0.065 (0.081)\tData 0.002 (0.005)\tLoss 0.2823 (0.2551)\tPrec 91.406% (90.865%)\n",
      "Epoch: [63][200/391]\tTime 0.079 (0.079)\tData 0.002 (0.003)\tLoss 0.2019 (0.2613)\tPrec 93.750% (90.761%)\n",
      "Epoch: [63][300/391]\tTime 0.078 (0.080)\tData 0.002 (0.003)\tLoss 0.2755 (0.2607)\tPrec 91.406% (90.822%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.236 (0.236)\tLoss 0.3914 (0.3914)\tPrec 85.938% (85.938%)\n",
      " * Prec 85.020% \n",
      "best acc: 86.230000\n",
      "Epoch: [64][0/391]\tTime 0.328 (0.328)\tData 0.244 (0.244)\tLoss 0.2245 (0.2245)\tPrec 92.188% (92.188%)\n",
      "Epoch: [64][100/391]\tTime 0.067 (0.085)\tData 0.002 (0.005)\tLoss 0.3803 (0.2552)\tPrec 87.500% (90.780%)\n",
      "Epoch: [64][200/391]\tTime 0.086 (0.079)\tData 0.002 (0.003)\tLoss 0.2371 (0.2645)\tPrec 91.406% (90.629%)\n",
      "Epoch: [64][300/391]\tTime 0.081 (0.080)\tData 0.002 (0.003)\tLoss 0.3310 (0.2701)\tPrec 88.281% (90.404%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.253 (0.253)\tLoss 0.4169 (0.4169)\tPrec 89.844% (89.844%)\n",
      " * Prec 84.160% \n",
      "best acc: 86.230000\n",
      "Epoch: [65][0/391]\tTime 0.322 (0.322)\tData 0.257 (0.257)\tLoss 0.2002 (0.2002)\tPrec 93.750% (93.750%)\n",
      "Epoch: [65][100/391]\tTime 0.087 (0.084)\tData 0.002 (0.005)\tLoss 0.3257 (0.2528)\tPrec 87.500% (91.019%)\n",
      "Epoch: [65][200/391]\tTime 0.085 (0.080)\tData 0.002 (0.004)\tLoss 0.4172 (0.2581)\tPrec 86.719% (90.819%)\n",
      "Epoch: [65][300/391]\tTime 0.083 (0.081)\tData 0.003 (0.003)\tLoss 0.2254 (0.2628)\tPrec 91.406% (90.596%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.223 (0.223)\tLoss 0.4165 (0.4165)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.900% \n",
      "best acc: 86.230000\n",
      "Epoch: [66][0/391]\tTime 0.288 (0.288)\tData 0.231 (0.231)\tLoss 0.1538 (0.1538)\tPrec 95.312% (95.312%)\n",
      "Epoch: [66][100/391]\tTime 0.073 (0.082)\tData 0.002 (0.004)\tLoss 0.2696 (0.2452)\tPrec 91.406% (91.344%)\n",
      "Epoch: [66][200/391]\tTime 0.067 (0.079)\tData 0.002 (0.003)\tLoss 0.2191 (0.2515)\tPrec 90.625% (91.091%)\n",
      "Epoch: [66][300/391]\tTime 0.083 (0.079)\tData 0.002 (0.003)\tLoss 0.3249 (0.2536)\tPrec 89.844% (90.996%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.262 (0.262)\tLoss 0.3880 (0.3880)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.460% \n",
      "best acc: 86.230000\n",
      "Epoch: [67][0/391]\tTime 0.302 (0.302)\tData 0.242 (0.242)\tLoss 0.3017 (0.3017)\tPrec 88.281% (88.281%)\n",
      "Epoch: [67][100/391]\tTime 0.086 (0.086)\tData 0.002 (0.005)\tLoss 0.2645 (0.2515)\tPrec 90.625% (91.197%)\n",
      "Epoch: [67][200/391]\tTime 0.088 (0.085)\tData 0.002 (0.004)\tLoss 0.2470 (0.2550)\tPrec 90.625% (91.095%)\n",
      "Epoch: [67][300/391]\tTime 0.081 (0.081)\tData 0.002 (0.003)\tLoss 0.2449 (0.2574)\tPrec 89.062% (90.944%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.266 (0.266)\tLoss 0.3604 (0.3604)\tPrec 88.281% (88.281%)\n",
      " * Prec 84.930% \n",
      "best acc: 86.230000\n",
      "Epoch: [68][0/391]\tTime 0.291 (0.291)\tData 0.239 (0.239)\tLoss 0.2940 (0.2940)\tPrec 89.062% (89.062%)\n",
      "Epoch: [68][100/391]\tTime 0.079 (0.078)\tData 0.002 (0.004)\tLoss 0.2204 (0.2544)\tPrec 95.312% (91.159%)\n",
      "Epoch: [68][200/391]\tTime 0.079 (0.080)\tData 0.002 (0.003)\tLoss 0.1652 (0.2525)\tPrec 93.750% (91.138%)\n",
      "Epoch: [68][300/391]\tTime 0.056 (0.079)\tData 0.002 (0.003)\tLoss 0.2426 (0.2551)\tPrec 92.188% (90.975%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.243 (0.243)\tLoss 0.3787 (0.3787)\tPrec 88.281% (88.281%)\n",
      " * Prec 83.850% \n",
      "best acc: 86.230000\n",
      "Epoch: [69][0/391]\tTime 0.330 (0.330)\tData 0.261 (0.261)\tLoss 0.2029 (0.2029)\tPrec 92.188% (92.188%)\n",
      "Epoch: [69][100/391]\tTime 0.067 (0.085)\tData 0.002 (0.005)\tLoss 0.1718 (0.2408)\tPrec 95.312% (91.484%)\n",
      "Epoch: [69][200/391]\tTime 0.085 (0.084)\tData 0.003 (0.004)\tLoss 0.2164 (0.2439)\tPrec 92.969% (91.305%)\n",
      "Epoch: [69][300/391]\tTime 0.080 (0.084)\tData 0.002 (0.003)\tLoss 0.2718 (0.2513)\tPrec 90.625% (90.970%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.243 (0.243)\tLoss 0.3714 (0.3714)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.810% \n",
      "best acc: 86.230000\n",
      "Epoch: [70][0/391]\tTime 0.308 (0.308)\tData 0.236 (0.236)\tLoss 0.2381 (0.2381)\tPrec 89.062% (89.062%)\n",
      "Epoch: [70][100/391]\tTime 0.085 (0.084)\tData 0.002 (0.005)\tLoss 0.3213 (0.2367)\tPrec 88.281% (91.669%)\n",
      "Epoch: [70][200/391]\tTime 0.080 (0.084)\tData 0.003 (0.003)\tLoss 0.2560 (0.2392)\tPrec 89.844% (91.535%)\n",
      "Epoch: [70][300/391]\tTime 0.085 (0.084)\tData 0.002 (0.003)\tLoss 0.2351 (0.2441)\tPrec 90.625% (91.414%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.329 (0.329)\tLoss 0.3232 (0.3232)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.760% \n",
      "best acc: 86.230000\n",
      "Epoch: [71][0/391]\tTime 0.287 (0.287)\tData 0.206 (0.206)\tLoss 0.2526 (0.2526)\tPrec 92.188% (92.188%)\n",
      "Epoch: [71][100/391]\tTime 0.084 (0.085)\tData 0.002 (0.004)\tLoss 0.2252 (0.2323)\tPrec 92.188% (91.979%)\n",
      "Epoch: [71][200/391]\tTime 0.089 (0.084)\tData 0.002 (0.003)\tLoss 0.2112 (0.2433)\tPrec 91.406% (91.445%)\n",
      "Epoch: [71][300/391]\tTime 0.085 (0.084)\tData 0.002 (0.003)\tLoss 0.1857 (0.2487)\tPrec 93.750% (91.315%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.235 (0.235)\tLoss 0.3877 (0.3877)\tPrec 85.938% (85.938%)\n",
      " * Prec 82.970% \n",
      "best acc: 86.230000\n",
      "Epoch: [72][0/391]\tTime 0.311 (0.311)\tData 0.240 (0.240)\tLoss 0.3744 (0.3744)\tPrec 87.500% (87.500%)\n",
      "Epoch: [72][100/391]\tTime 0.096 (0.082)\tData 0.006 (0.005)\tLoss 0.2136 (0.2541)\tPrec 90.625% (91.136%)\n",
      "Epoch: [72][200/391]\tTime 0.078 (0.083)\tData 0.002 (0.004)\tLoss 0.2824 (0.2536)\tPrec 89.844% (91.111%)\n",
      "Epoch: [72][300/391]\tTime 0.085 (0.083)\tData 0.003 (0.003)\tLoss 0.3748 (0.2491)\tPrec 86.719% (91.240%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.240 (0.240)\tLoss 0.3386 (0.3386)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.670% \n",
      "best acc: 86.230000\n",
      "Epoch: [73][0/391]\tTime 0.323 (0.323)\tData 0.271 (0.271)\tLoss 0.1478 (0.1478)\tPrec 95.312% (95.312%)\n",
      "Epoch: [73][100/391]\tTime 0.081 (0.077)\tData 0.002 (0.005)\tLoss 0.2482 (0.2318)\tPrec 90.625% (91.894%)\n",
      "Epoch: [73][200/391]\tTime 0.090 (0.078)\tData 0.002 (0.004)\tLoss 0.1715 (0.2344)\tPrec 95.312% (91.818%)\n",
      "Epoch: [73][300/391]\tTime 0.088 (0.080)\tData 0.002 (0.003)\tLoss 0.1867 (0.2384)\tPrec 92.969% (91.658%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.236 (0.236)\tLoss 0.3209 (0.3209)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.380% \n",
      "best acc: 86.380000\n",
      "Epoch: [74][0/391]\tTime 0.334 (0.334)\tData 0.274 (0.274)\tLoss 0.1472 (0.1472)\tPrec 95.312% (95.312%)\n",
      "Epoch: [74][100/391]\tTime 0.090 (0.075)\tData 0.002 (0.005)\tLoss 0.2155 (0.2381)\tPrec 89.844% (91.576%)\n",
      "Epoch: [74][200/391]\tTime 0.077 (0.078)\tData 0.002 (0.004)\tLoss 0.3046 (0.2413)\tPrec 87.500% (91.379%)\n",
      "Epoch: [74][300/391]\tTime 0.090 (0.080)\tData 0.002 (0.003)\tLoss 0.2084 (0.2464)\tPrec 92.188% (91.274%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.246 (0.246)\tLoss 0.6060 (0.6060)\tPrec 83.594% (83.594%)\n",
      " * Prec 81.540% \n",
      "best acc: 86.380000\n",
      "Epoch: [75][0/391]\tTime 0.290 (0.290)\tData 0.225 (0.225)\tLoss 0.2172 (0.2172)\tPrec 89.844% (89.844%)\n",
      "Epoch: [75][100/391]\tTime 0.067 (0.081)\tData 0.003 (0.005)\tLoss 0.2953 (0.2354)\tPrec 87.500% (91.515%)\n",
      "Epoch: [75][200/391]\tTime 0.075 (0.079)\tData 0.002 (0.003)\tLoss 0.1826 (0.2394)\tPrec 93.750% (91.531%)\n",
      "Epoch: [75][300/391]\tTime 0.087 (0.080)\tData 0.003 (0.003)\tLoss 0.2468 (0.2404)\tPrec 88.281% (91.430%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.267 (0.267)\tLoss 0.4388 (0.4388)\tPrec 84.375% (84.375%)\n",
      " * Prec 84.650% \n",
      "best acc: 86.380000\n",
      "Epoch: [76][0/391]\tTime 0.352 (0.352)\tData 0.288 (0.288)\tLoss 0.2398 (0.2398)\tPrec 92.969% (92.969%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [76][100/391]\tTime 0.087 (0.087)\tData 0.003 (0.005)\tLoss 0.2522 (0.2429)\tPrec 89.062% (91.491%)\n",
      "Epoch: [76][200/391]\tTime 0.044 (0.076)\tData 0.002 (0.004)\tLoss 0.3810 (0.2368)\tPrec 85.938% (91.589%)\n",
      "Epoch: [76][300/391]\tTime 0.078 (0.070)\tData 0.003 (0.003)\tLoss 0.3238 (0.2391)\tPrec 89.062% (91.507%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.222 (0.222)\tLoss 0.4441 (0.4441)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.090% \n",
      "best acc: 86.380000\n",
      "Epoch: [77][0/391]\tTime 0.311 (0.311)\tData 0.238 (0.238)\tLoss 0.2383 (0.2383)\tPrec 94.531% (94.531%)\n",
      "Epoch: [77][100/391]\tTime 0.067 (0.063)\tData 0.002 (0.005)\tLoss 0.2346 (0.2310)\tPrec 89.844% (91.901%)\n",
      "Epoch: [77][200/391]\tTime 0.069 (0.064)\tData 0.004 (0.004)\tLoss 0.3094 (0.2367)\tPrec 87.500% (91.737%)\n",
      "Epoch: [77][300/391]\tTime 0.064 (0.063)\tData 0.002 (0.003)\tLoss 0.1871 (0.2385)\tPrec 93.750% (91.609%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.220 (0.220)\tLoss 0.3377 (0.3377)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.230% \n",
      "best acc: 86.380000\n",
      "Epoch: [78][0/391]\tTime 0.319 (0.319)\tData 0.257 (0.257)\tLoss 0.2471 (0.2471)\tPrec 91.406% (91.406%)\n",
      "Epoch: [78][100/391]\tTime 0.060 (0.067)\tData 0.002 (0.005)\tLoss 0.1923 (0.2304)\tPrec 92.969% (92.056%)\n",
      "Epoch: [78][200/391]\tTime 0.058 (0.066)\tData 0.002 (0.004)\tLoss 0.2317 (0.2394)\tPrec 95.312% (91.535%)\n",
      "Epoch: [78][300/391]\tTime 0.067 (0.064)\tData 0.002 (0.003)\tLoss 0.3307 (0.2415)\tPrec 88.281% (91.500%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.227 (0.227)\tLoss 0.5124 (0.5124)\tPrec 84.375% (84.375%)\n",
      " * Prec 83.810% \n",
      "best acc: 86.380000\n",
      "Epoch: [79][0/391]\tTime 0.350 (0.350)\tData 0.280 (0.280)\tLoss 0.3058 (0.3058)\tPrec 91.406% (91.406%)\n",
      "Epoch: [79][100/391]\tTime 0.063 (0.066)\tData 0.002 (0.005)\tLoss 0.1925 (0.2202)\tPrec 92.188% (92.257%)\n",
      "Epoch: [79][200/391]\tTime 0.061 (0.064)\tData 0.002 (0.004)\tLoss 0.1927 (0.2288)\tPrec 94.531% (92.001%)\n",
      "Epoch: [79][300/391]\tTime 0.053 (0.064)\tData 0.002 (0.003)\tLoss 0.2741 (0.2305)\tPrec 89.844% (91.816%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.249 (0.249)\tLoss 0.4023 (0.4023)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.550% \n",
      "best acc: 86.380000\n",
      "Epoch: [80][0/391]\tTime 0.243 (0.243)\tData 0.192 (0.192)\tLoss 0.3372 (0.3372)\tPrec 90.625% (90.625%)\n",
      "Epoch: [80][100/391]\tTime 0.064 (0.062)\tData 0.002 (0.004)\tLoss 0.1812 (0.1944)\tPrec 94.531% (93.015%)\n",
      "Epoch: [80][200/391]\tTime 0.071 (0.060)\tData 0.002 (0.003)\tLoss 0.1008 (0.1810)\tPrec 96.875% (93.560%)\n",
      "Epoch: [80][300/391]\tTime 0.051 (0.062)\tData 0.002 (0.003)\tLoss 0.0748 (0.1767)\tPrec 97.656% (93.805%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.231 (0.231)\tLoss 0.2854 (0.2854)\tPrec 88.281% (88.281%)\n",
      " * Prec 89.320% \n",
      "best acc: 89.320000\n",
      "Epoch: [81][0/391]\tTime 0.289 (0.289)\tData 0.228 (0.228)\tLoss 0.1401 (0.1401)\tPrec 96.094% (96.094%)\n",
      "Epoch: [81][100/391]\tTime 0.058 (0.068)\tData 0.002 (0.005)\tLoss 0.1627 (0.1482)\tPrec 94.531% (95.026%)\n",
      "Epoch: [81][200/391]\tTime 0.055 (0.065)\tData 0.002 (0.003)\tLoss 0.1860 (0.1482)\tPrec 92.969% (95.040%)\n",
      "Epoch: [81][300/391]\tTime 0.057 (0.064)\tData 0.002 (0.003)\tLoss 0.1648 (0.1495)\tPrec 93.750% (94.970%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.239 (0.239)\tLoss 0.2537 (0.2537)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.240% \n",
      "best acc: 89.320000\n",
      "Epoch: [82][0/391]\tTime 0.284 (0.284)\tData 0.216 (0.216)\tLoss 0.2043 (0.2043)\tPrec 93.750% (93.750%)\n",
      "Epoch: [82][100/391]\tTime 0.065 (0.062)\tData 0.002 (0.004)\tLoss 0.1665 (0.1414)\tPrec 94.531% (95.104%)\n",
      "Epoch: [82][200/391]\tTime 0.061 (0.063)\tData 0.002 (0.003)\tLoss 0.1258 (0.1397)\tPrec 96.875% (95.254%)\n",
      "Epoch: [82][300/391]\tTime 0.067 (0.062)\tData 0.003 (0.003)\tLoss 0.1327 (0.1416)\tPrec 94.531% (95.149%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.220 (0.220)\tLoss 0.2261 (0.2261)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.370% \n",
      "best acc: 89.370000\n",
      "Epoch: [83][0/391]\tTime 0.274 (0.274)\tData 0.218 (0.218)\tLoss 0.1223 (0.1223)\tPrec 95.312% (95.312%)\n",
      "Epoch: [83][100/391]\tTime 0.055 (0.060)\tData 0.002 (0.004)\tLoss 0.1479 (0.1396)\tPrec 92.188% (95.204%)\n",
      "Epoch: [83][200/391]\tTime 0.071 (0.061)\tData 0.002 (0.003)\tLoss 0.1604 (0.1392)\tPrec 96.094% (95.239%)\n",
      "Epoch: [83][300/391]\tTime 0.054 (0.061)\tData 0.002 (0.003)\tLoss 0.1091 (0.1386)\tPrec 96.094% (95.250%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.223 (0.223)\tLoss 0.2681 (0.2681)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.480% \n",
      "best acc: 89.480000\n",
      "Epoch: [84][0/391]\tTime 0.314 (0.314)\tData 0.244 (0.244)\tLoss 0.0900 (0.0900)\tPrec 99.219% (99.219%)\n",
      "Epoch: [84][100/391]\tTime 0.051 (0.066)\tData 0.002 (0.005)\tLoss 0.1338 (0.1264)\tPrec 96.094% (95.707%)\n",
      "Epoch: [84][200/391]\tTime 0.063 (0.063)\tData 0.002 (0.003)\tLoss 0.1478 (0.1271)\tPrec 95.312% (95.713%)\n",
      "Epoch: [84][300/391]\tTime 0.072 (0.064)\tData 0.002 (0.003)\tLoss 0.1765 (0.1306)\tPrec 95.312% (95.515%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.239 (0.239)\tLoss 0.2298 (0.2298)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.420% \n",
      "best acc: 89.480000\n",
      "Epoch: [85][0/391]\tTime 0.292 (0.292)\tData 0.237 (0.237)\tLoss 0.1516 (0.1516)\tPrec 95.312% (95.312%)\n",
      "Epoch: [85][100/391]\tTime 0.053 (0.064)\tData 0.002 (0.005)\tLoss 0.1621 (0.1246)\tPrec 94.531% (95.730%)\n",
      "Epoch: [85][200/391]\tTime 0.073 (0.064)\tData 0.002 (0.003)\tLoss 0.0629 (0.1270)\tPrec 99.219% (95.693%)\n",
      "Epoch: [85][300/391]\tTime 0.057 (0.066)\tData 0.002 (0.003)\tLoss 0.1282 (0.1274)\tPrec 95.312% (95.673%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.254 (0.254)\tLoss 0.2179 (0.2179)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.600% \n",
      "best acc: 89.600000\n",
      "Epoch: [86][0/391]\tTime 0.312 (0.312)\tData 0.243 (0.243)\tLoss 0.1148 (0.1148)\tPrec 96.094% (96.094%)\n",
      "Epoch: [86][100/391]\tTime 0.071 (0.053)\tData 0.002 (0.004)\tLoss 0.1514 (0.1251)\tPrec 92.969% (95.645%)\n",
      "Epoch: [86][200/391]\tTime 0.058 (0.059)\tData 0.002 (0.003)\tLoss 0.0802 (0.1260)\tPrec 97.656% (95.600%)\n",
      "Epoch: [86][300/391]\tTime 0.065 (0.060)\tData 0.003 (0.003)\tLoss 0.1115 (0.1272)\tPrec 96.875% (95.616%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.243 (0.243)\tLoss 0.2843 (0.2843)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.400% \n",
      "best acc: 89.600000\n",
      "Epoch: [87][0/391]\tTime 0.317 (0.317)\tData 0.251 (0.251)\tLoss 0.1418 (0.1418)\tPrec 92.188% (92.188%)\n",
      "Epoch: [87][100/391]\tTime 0.056 (0.063)\tData 0.002 (0.005)\tLoss 0.0734 (0.1197)\tPrec 96.875% (95.838%)\n",
      "Epoch: [87][200/391]\tTime 0.048 (0.057)\tData 0.002 (0.003)\tLoss 0.1662 (0.1224)\tPrec 95.312% (95.791%)\n",
      "Epoch: [87][300/391]\tTime 0.065 (0.057)\tData 0.002 (0.003)\tLoss 0.1300 (0.1237)\tPrec 94.531% (95.699%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.224 (0.224)\tLoss 0.2394 (0.2394)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.740% \n",
      "best acc: 89.740000\n",
      "Epoch: [88][0/391]\tTime 0.304 (0.304)\tData 0.237 (0.237)\tLoss 0.0756 (0.0756)\tPrec 98.438% (98.438%)\n",
      "Epoch: [88][100/391]\tTime 0.074 (0.067)\tData 0.002 (0.005)\tLoss 0.1205 (0.1249)\tPrec 93.750% (95.552%)\n",
      "Epoch: [88][200/391]\tTime 0.043 (0.063)\tData 0.002 (0.003)\tLoss 0.1262 (0.1239)\tPrec 95.312% (95.608%)\n",
      "Epoch: [88][300/391]\tTime 0.054 (0.062)\tData 0.002 (0.003)\tLoss 0.0874 (0.1221)\tPrec 98.438% (95.777%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.218 (0.218)\tLoss 0.2316 (0.2316)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.870% \n",
      "best acc: 89.870000\n",
      "Epoch: [89][0/391]\tTime 0.290 (0.290)\tData 0.231 (0.231)\tLoss 0.0626 (0.0626)\tPrec 98.438% (98.438%)\n",
      "Epoch: [89][100/391]\tTime 0.073 (0.069)\tData 0.002 (0.005)\tLoss 0.2100 (0.1129)\tPrec 92.188% (96.202%)\n",
      "Epoch: [89][200/391]\tTime 0.050 (0.067)\tData 0.002 (0.003)\tLoss 0.0787 (0.1140)\tPrec 97.656% (96.125%)\n",
      "Epoch: [89][300/391]\tTime 0.061 (0.066)\tData 0.002 (0.003)\tLoss 0.1370 (0.1176)\tPrec 96.875% (96.000%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.267 (0.267)\tLoss 0.2037 (0.2037)\tPrec 94.531% (94.531%)\n",
      " * Prec 89.720% \n",
      "best acc: 89.870000\n",
      "Epoch: [90][0/391]\tTime 0.298 (0.298)\tData 0.248 (0.248)\tLoss 0.1637 (0.1637)\tPrec 94.531% (94.531%)\n",
      "Epoch: [90][100/391]\tTime 0.067 (0.061)\tData 0.003 (0.005)\tLoss 0.1120 (0.1161)\tPrec 96.094% (95.862%)\n",
      "Epoch: [90][200/391]\tTime 0.055 (0.060)\tData 0.002 (0.003)\tLoss 0.0835 (0.1173)\tPrec 97.656% (95.845%)\n",
      "Epoch: [90][300/391]\tTime 0.046 (0.061)\tData 0.002 (0.003)\tLoss 0.1728 (0.1175)\tPrec 92.969% (95.902%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.226 (0.226)\tLoss 0.2198 (0.2198)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.680% \n",
      "best acc: 89.870000\n",
      "Epoch: [91][0/391]\tTime 0.313 (0.313)\tData 0.243 (0.243)\tLoss 0.0764 (0.0764)\tPrec 98.438% (98.438%)\n",
      "Epoch: [91][100/391]\tTime 0.042 (0.064)\tData 0.001 (0.005)\tLoss 0.1652 (0.1150)\tPrec 95.312% (96.063%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [91][200/391]\tTime 0.058 (0.060)\tData 0.002 (0.003)\tLoss 0.1019 (0.1137)\tPrec 95.312% (96.067%)\n",
      "Epoch: [91][300/391]\tTime 0.053 (0.059)\tData 0.002 (0.003)\tLoss 0.1452 (0.1140)\tPrec 96.875% (96.021%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.215 (0.215)\tLoss 0.2292 (0.2292)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.440% \n",
      "best acc: 89.870000\n",
      "Epoch: [92][0/391]\tTime 0.309 (0.309)\tData 0.243 (0.243)\tLoss 0.0701 (0.0701)\tPrec 98.438% (98.438%)\n",
      "Epoch: [92][100/391]\tTime 0.066 (0.056)\tData 0.002 (0.004)\tLoss 0.0794 (0.1113)\tPrec 97.656% (96.272%)\n",
      "Epoch: [92][200/391]\tTime 0.066 (0.059)\tData 0.002 (0.003)\tLoss 0.0480 (0.1160)\tPrec 99.219% (96.137%)\n",
      "Epoch: [92][300/391]\tTime 0.056 (0.060)\tData 0.002 (0.003)\tLoss 0.1624 (0.1168)\tPrec 94.531% (96.070%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.232 (0.232)\tLoss 0.2576 (0.2576)\tPrec 89.844% (89.844%)\n",
      " * Prec 89.590% \n",
      "best acc: 89.870000\n",
      "Epoch: [93][0/391]\tTime 0.297 (0.297)\tData 0.227 (0.227)\tLoss 0.1147 (0.1147)\tPrec 94.531% (94.531%)\n",
      "Epoch: [93][100/391]\tTime 0.067 (0.067)\tData 0.002 (0.004)\tLoss 0.1527 (0.1092)\tPrec 92.969% (96.156%)\n",
      "Epoch: [93][200/391]\tTime 0.053 (0.062)\tData 0.002 (0.003)\tLoss 0.1430 (0.1101)\tPrec 94.531% (96.230%)\n",
      "Epoch: [93][300/391]\tTime 0.046 (0.060)\tData 0.002 (0.003)\tLoss 0.1045 (0.1124)\tPrec 95.312% (96.122%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.257 (0.257)\tLoss 0.2267 (0.2267)\tPrec 93.750% (93.750%)\n",
      " * Prec 89.710% \n",
      "best acc: 89.870000\n",
      "Epoch: [94][0/391]\tTime 0.314 (0.314)\tData 0.257 (0.257)\tLoss 0.0776 (0.0776)\tPrec 97.656% (97.656%)\n",
      "Epoch: [94][100/391]\tTime 0.056 (0.062)\tData 0.002 (0.005)\tLoss 0.1163 (0.1133)\tPrec 95.312% (96.009%)\n",
      "Epoch: [94][200/391]\tTime 0.076 (0.056)\tData 0.002 (0.003)\tLoss 0.1035 (0.1107)\tPrec 97.656% (96.117%)\n",
      "Epoch: [94][300/391]\tTime 0.058 (0.058)\tData 0.002 (0.003)\tLoss 0.0588 (0.1098)\tPrec 97.656% (96.211%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.237 (0.237)\tLoss 0.1803 (0.1803)\tPrec 96.094% (96.094%)\n",
      " * Prec 89.550% \n",
      "best acc: 89.870000\n",
      "Epoch: [95][0/391]\tTime 0.310 (0.310)\tData 0.235 (0.235)\tLoss 0.0578 (0.0578)\tPrec 97.656% (97.656%)\n",
      "Epoch: [95][100/391]\tTime 0.061 (0.060)\tData 0.002 (0.004)\tLoss 0.0900 (0.1083)\tPrec 96.094% (96.310%)\n",
      "Epoch: [95][200/391]\tTime 0.065 (0.064)\tData 0.002 (0.003)\tLoss 0.0775 (0.1084)\tPrec 97.656% (96.276%)\n",
      "Epoch: [95][300/391]\tTime 0.065 (0.061)\tData 0.003 (0.003)\tLoss 0.1470 (0.1101)\tPrec 96.094% (96.195%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.266 (0.266)\tLoss 0.2326 (0.2326)\tPrec 89.844% (89.844%)\n",
      " * Prec 89.660% \n",
      "best acc: 89.870000\n",
      "Epoch: [96][0/391]\tTime 0.278 (0.278)\tData 0.216 (0.216)\tLoss 0.1168 (0.1168)\tPrec 95.312% (95.312%)\n",
      "Epoch: [96][100/391]\tTime 0.052 (0.073)\tData 0.002 (0.004)\tLoss 0.1460 (0.1025)\tPrec 94.531% (96.364%)\n",
      "Epoch: [96][200/391]\tTime 0.054 (0.064)\tData 0.002 (0.003)\tLoss 0.1175 (0.1051)\tPrec 96.875% (96.245%)\n",
      "Epoch: [96][300/391]\tTime 0.060 (0.060)\tData 0.002 (0.003)\tLoss 0.1030 (0.1077)\tPrec 96.094% (96.203%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.231 (0.231)\tLoss 0.2733 (0.2733)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.470% \n",
      "best acc: 89.870000\n",
      "Epoch: [97][0/391]\tTime 0.269 (0.269)\tData 0.204 (0.204)\tLoss 0.0813 (0.0813)\tPrec 96.875% (96.875%)\n",
      "Epoch: [97][100/391]\tTime 0.043 (0.065)\tData 0.002 (0.004)\tLoss 0.0610 (0.1033)\tPrec 98.438% (96.334%)\n",
      "Epoch: [97][200/391]\tTime 0.054 (0.062)\tData 0.002 (0.003)\tLoss 0.0786 (0.1088)\tPrec 96.094% (96.133%)\n",
      "Epoch: [97][300/391]\tTime 0.062 (0.063)\tData 0.002 (0.003)\tLoss 0.0825 (0.1085)\tPrec 96.094% (96.190%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.230 (0.230)\tLoss 0.2486 (0.2486)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.320% \n",
      "best acc: 89.870000\n",
      "Epoch: [98][0/391]\tTime 0.304 (0.304)\tData 0.240 (0.240)\tLoss 0.0677 (0.0677)\tPrec 98.438% (98.438%)\n",
      "Epoch: [98][100/391]\tTime 0.070 (0.066)\tData 0.002 (0.005)\tLoss 0.1387 (0.1071)\tPrec 93.750% (96.612%)\n",
      "Epoch: [98][200/391]\tTime 0.044 (0.062)\tData 0.002 (0.003)\tLoss 0.1152 (0.1086)\tPrec 96.094% (96.319%)\n",
      "Epoch: [98][300/391]\tTime 0.059 (0.061)\tData 0.002 (0.003)\tLoss 0.0771 (0.1099)\tPrec 97.656% (96.268%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.253 (0.253)\tLoss 0.2511 (0.2511)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.500% \n",
      "best acc: 89.870000\n",
      "Epoch: [99][0/391]\tTime 0.334 (0.334)\tData 0.258 (0.258)\tLoss 0.1006 (0.1006)\tPrec 96.875% (96.875%)\n",
      "Epoch: [99][100/391]\tTime 0.068 (0.068)\tData 0.002 (0.005)\tLoss 0.1059 (0.0989)\tPrec 96.094% (96.604%)\n",
      "Epoch: [99][200/391]\tTime 0.047 (0.067)\tData 0.002 (0.004)\tLoss 0.1471 (0.1042)\tPrec 93.750% (96.436%)\n",
      "Epoch: [99][300/391]\tTime 0.056 (0.066)\tData 0.003 (0.003)\tLoss 0.0421 (0.1030)\tPrec 98.438% (96.465%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.245 (0.245)\tLoss 0.2459 (0.2459)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.870% \n",
      "best acc: 89.870000\n"
     ]
    }
   ],
   "source": [
    "# This cell won't be given, but students will complete the training\n",
    "\n",
    "lr = 4e-2\n",
    "weight_decay = 1e-4\n",
    "epochs = 100\n",
    "best_prec = 0\n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "fdir = 'result/'+str(model_name)\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "        \n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on test set\n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "\n",
    "    # remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b081a0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n"
     ]
    }
   ],
   "source": [
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []  \n",
    "        \n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\") \n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        print(\"prehooked\")\n",
    "        layer.register_forward_pre_hook(save_output)       ## Input for the module will be grapped       \n",
    "####################################################\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.to(device)\n",
    "out = model(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW\n",
    "\n",
    "#  1. Train with 4 bits for both weight and activation to achieve >90% accuracy\n",
    "#  2. Find x_int and w_int for the 2nd convolution layer\n",
    "#  3. Check the recovered psum has similar value to the un-quantized original psum\n",
    "#     (such as example 1 in W3S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 8987/10000 (90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = \"result/resnet20_quant4bit/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(save_output.outputs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "spoken-worst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0., -1., -1.],\n",
      "          [-0., -2.,  3.],\n",
      "          [-0., -2., -3.]],\n",
      "\n",
      "         [[ 2., -2.,  2.],\n",
      "          [-2., -2., -0.],\n",
      "          [-0., -2., -3.]],\n",
      "\n",
      "         [[-5., -1.,  7.],\n",
      "          [ 1.,  1.,  7.],\n",
      "          [-4.,  0.,  3.]],\n",
      "\n",
      "         [[ 0., -2., -3.],\n",
      "          [ 0., -3.,  0.],\n",
      "          [ 0., -1., -1.]],\n",
      "\n",
      "         [[-2., -2., -3.],\n",
      "          [ 4., -7., -3.],\n",
      "          [-0., -2., -3.]],\n",
      "\n",
      "         [[ 1., -1., -2.],\n",
      "          [-1.,  3., -2.],\n",
      "          [ 3., -1., -1.]],\n",
      "\n",
      "         [[ 1.,  1., -3.],\n",
      "          [ 0., -5.,  6.],\n",
      "          [ 1., -2., -0.]],\n",
      "\n",
      "         [[ 2., -2., -1.],\n",
      "          [ 1.,  4., -0.],\n",
      "          [ 1., -0.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.,  1.,  1.],\n",
      "          [-1., -1., -1.],\n",
      "          [-0., -3., -3.]],\n",
      "\n",
      "         [[-1.,  2.,  1.],\n",
      "          [ 0.,  1.,  5.],\n",
      "          [-1.,  0., -0.]],\n",
      "\n",
      "         [[-3.,  0., -2.],\n",
      "          [-1.,  6., -0.],\n",
      "          [-0.,  4.,  2.]],\n",
      "\n",
      "         [[ 3.,  2.,  0.],\n",
      "          [-2., -5., -5.],\n",
      "          [ 0., -2., -0.]],\n",
      "\n",
      "         [[ 3.,  2., -0.],\n",
      "          [ 1.,  6.,  0.],\n",
      "          [-5.,  4., -0.]],\n",
      "\n",
      "         [[ 2.,  1., -0.],\n",
      "          [ 1.,  0.,  1.],\n",
      "          [ 4.,  0., -2.]],\n",
      "\n",
      "         [[ 0., -1.,  3.],\n",
      "          [-0., -6., -4.],\n",
      "          [-1.,  2.,  2.]],\n",
      "\n",
      "         [[ 0.,  2.,  1.],\n",
      "          [ 4.,  2.,  0.],\n",
      "          [ 0.,  0., -3.]]],\n",
      "\n",
      "\n",
      "        [[[-6.,  1., -0.],\n",
      "          [ 3., -2., -2.],\n",
      "          [-0., -2., -2.]],\n",
      "\n",
      "         [[-1.,  0., -0.],\n",
      "          [ 3.,  1., -4.],\n",
      "          [ 2.,  0., -3.]],\n",
      "\n",
      "         [[-3.,  6.,  2.],\n",
      "          [ 2., -7.,  2.],\n",
      "          [ 1., -4., -2.]],\n",
      "\n",
      "         [[-3., -0.,  3.],\n",
      "          [-3., -0.,  3.],\n",
      "          [-2., -1.,  1.]],\n",
      "\n",
      "         [[-7., -3.,  1.],\n",
      "          [ 2.,  1., -1.],\n",
      "          [ 4.,  0.,  0.]],\n",
      "\n",
      "         [[ 1., -3.,  2.],\n",
      "          [-1., -1.,  1.],\n",
      "          [-1.,  2., -1.]],\n",
      "\n",
      "         [[-7.,  6.,  3.],\n",
      "          [-7.,  2.,  1.],\n",
      "          [ 7., -4., -2.]],\n",
      "\n",
      "         [[ 3.,  4., -3.],\n",
      "          [ 2., -1.,  2.],\n",
      "          [-2., -4., -0.]]],\n",
      "\n",
      "\n",
      "        [[[-0.,  1.,  2.],\n",
      "          [ 0., -3., -1.],\n",
      "          [-1., -1.,  0.]],\n",
      "\n",
      "         [[ 2., -0., -1.],\n",
      "          [ 1., -2., -3.],\n",
      "          [ 0.,  1.,  2.]],\n",
      "\n",
      "         [[ 3., -1.,  1.],\n",
      "          [ 3.,  0.,  3.],\n",
      "          [ 1.,  0., -0.]],\n",
      "\n",
      "         [[-2., -1.,  0.],\n",
      "          [ 1.,  1.,  2.],\n",
      "          [ 0.,  1.,  2.]],\n",
      "\n",
      "         [[ 1.,  1.,  2.],\n",
      "          [-2., -5., -1.],\n",
      "          [-1., -1.,  2.]],\n",
      "\n",
      "         [[ 2., -1.,  3.],\n",
      "          [-2., -3., -2.],\n",
      "          [-3., -5., -5.]],\n",
      "\n",
      "         [[-1.,  0., -1.],\n",
      "          [ 2., -3.,  2.],\n",
      "          [ 2.,  2.,  4.]],\n",
      "\n",
      "         [[ 0., -0.,  3.],\n",
      "          [ 1.,  6.,  4.],\n",
      "          [-0.,  4.,  2.]]],\n",
      "\n",
      "\n",
      "        [[[-1.,  0.,  2.],\n",
      "          [ 2.,  3.,  2.],\n",
      "          [-1.,  5.,  0.]],\n",
      "\n",
      "         [[ 1.,  3.,  2.],\n",
      "          [ 0.,  5.,  1.],\n",
      "          [-4.,  1.,  3.]],\n",
      "\n",
      "         [[-0., -1.,  1.],\n",
      "          [ 3.,  1.,  1.],\n",
      "          [-0.,  3.,  1.]],\n",
      "\n",
      "         [[-3., -0.,  2.],\n",
      "          [-3., -0.,  1.],\n",
      "          [-3., -2.,  2.]],\n",
      "\n",
      "         [[ 0., -1., -2.],\n",
      "          [ 0.,  0.,  1.],\n",
      "          [ 1., -1.,  1.]],\n",
      "\n",
      "         [[ 1., -3.,  0.],\n",
      "          [-2.,  4., -0.],\n",
      "          [-2.,  4.,  1.]],\n",
      "\n",
      "         [[-1.,  2., -1.],\n",
      "          [ 3.,  2.,  1.],\n",
      "          [-1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1., -1., -0.],\n",
      "          [-1., -3., -1.],\n",
      "          [-1., -1., -2.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1.,  1.],\n",
      "          [-2., -3., -4.],\n",
      "          [-2., -3.,  1.]],\n",
      "\n",
      "         [[ 0., -0.,  2.],\n",
      "          [-0., -2., -3.],\n",
      "          [ 1., -5., -3.]],\n",
      "\n",
      "         [[ 0., -1., -4.],\n",
      "          [ 6.,  3., -7.],\n",
      "          [ 1.,  1., -1.]],\n",
      "\n",
      "         [[ 1., -2.,  1.],\n",
      "          [ 1., -2.,  1.],\n",
      "          [ 1.,  0., -0.]],\n",
      "\n",
      "         [[-1.,  2.,  2.],\n",
      "          [-5.,  6.,  3.],\n",
      "          [-3.,  5., -1.]],\n",
      "\n",
      "         [[-3.,  0., -0.],\n",
      "          [-1., -1., -0.],\n",
      "          [-2., -5.,  2.]],\n",
      "\n",
      "         [[ 1., -5.,  0.],\n",
      "          [ 3.,  6., -1.],\n",
      "          [ 3.,  7.,  0.]],\n",
      "\n",
      "         [[-1.,  2., -2.],\n",
      "          [-2., -1., -4.],\n",
      "          [-1., -1.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  4.,  1.],\n",
      "          [ 6.,  7.,  6.],\n",
      "          [-4., -2.,  2.]],\n",
      "\n",
      "         [[ 0.,  1., -0.],\n",
      "          [ 3.,  4.,  4.],\n",
      "          [-4., -4., -2.]],\n",
      "\n",
      "         [[-0.,  2., -1.],\n",
      "          [ 2.,  0.,  6.],\n",
      "          [-0., -4., -5.]],\n",
      "\n",
      "         [[ 1., -1., -2.],\n",
      "          [ 1.,  3.,  2.],\n",
      "          [ 0., -1., -4.]],\n",
      "\n",
      "         [[ 3.,  2., -2.],\n",
      "          [-7., -1.,  2.],\n",
      "          [ 2., -1., -4.]],\n",
      "\n",
      "         [[ 0.,  3., -1.],\n",
      "          [ 5., -5., -2.],\n",
      "          [-4.,  4., -2.]],\n",
      "\n",
      "         [[ 0., -4., -1.],\n",
      "          [ 1.,  7.,  6.],\n",
      "          [ 1., -7., -1.]],\n",
      "\n",
      "         [[-1.,  3.,  2.],\n",
      "          [-0., -3., -7.],\n",
      "          [-1.,  3.,  5.]]],\n",
      "\n",
      "\n",
      "        [[[ 0.,  0.,  2.],\n",
      "          [ 2.,  0., -1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1., -2.],\n",
      "          [ 1., -0., -2.],\n",
      "          [-1.,  2.,  0.]],\n",
      "\n",
      "         [[-1., -0., -3.],\n",
      "          [-2.,  3., -2.],\n",
      "          [-0., -1., -1.]],\n",
      "\n",
      "         [[-1., -4., -3.],\n",
      "          [-2., -4., -3.],\n",
      "          [-0., -2., -1.]],\n",
      "\n",
      "         [[ 1.,  2., -1.],\n",
      "          [-2.,  7.,  1.],\n",
      "          [ 1.,  1., -1.]],\n",
      "\n",
      "         [[ 1.,  3.,  1.],\n",
      "          [ 0., -6., -2.],\n",
      "          [-1., -4., -4.]],\n",
      "\n",
      "         [[ 4., -0.,  1.],\n",
      "          [ 1.,  5., -2.],\n",
      "          [ 0., -1., -1.]],\n",
      "\n",
      "         [[-1.,  3.,  2.],\n",
      "          [-1.,  3.,  0.],\n",
      "          [-0.,  3.,  3.]]]], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "w_bit = 4\n",
    "weight_q = model.layer1[0].conv2.weight_q # quantized value is stored during the training\n",
    "w_alpha = model.layer1[0].conv2.weight_quant.wgt_alpha\n",
    "w_delta = w_alpha/(2**(w_bit-1)-1)\n",
    "weight_int = weight_q/w_delta\n",
    "print(weight_int) # you should see clean integer numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "interior-oxygen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.,  0.,  0.,  ...,  3.,  2.,  1.],\n",
      "          [ 0.,  1.,  1.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 8.,  8.,  9.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  2.,  2.,  1.],\n",
      "          [ 1.,  2.,  2.,  ...,  2.,  2.,  1.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  3.,  3.,  2.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  1.],\n",
      "          [ 0.,  0.,  0.,  ...,  3.,  1.,  2.],\n",
      "          ...,\n",
      "          [ 0.,  7.,  5.,  ...,  2.,  1.,  2.],\n",
      "          [ 0.,  0.,  0.,  ...,  1.,  0.,  2.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 4.,  0.,  1.,  ..., 10.,  2.,  2.],\n",
      "          [ 1.,  0.,  2.,  ...,  8.,  2.,  2.],\n",
      "          [ 0.,  0.,  3.,  ...,  9.,  2.,  2.],\n",
      "          ...,\n",
      "          [ 8.,  3.,  4.,  ...,  1.,  2.,  2.],\n",
      "          [ 7.,  2.,  2.,  ...,  2.,  2.,  1.],\n",
      "          [ 4.,  3.,  2.,  ...,  2.,  2.,  2.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  1.,  0.,  0.],\n",
      "          [ 1.,  0.,  0.,  ...,  2.,  1.,  0.],\n",
      "          ...,\n",
      "          [ 2.,  2.,  2.,  ...,  1.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  1.,  1.,  1.],\n",
      "          [ 0.,  1.,  0.,  ...,  0.,  1.,  0.]],\n",
      "\n",
      "         [[ 4.,  3.,  2.,  ...,  6.,  4.,  4.],\n",
      "          [ 4.,  4.,  4.,  ...,  4.,  4.,  4.],\n",
      "          [ 2.,  3.,  4.,  ...,  4.,  4.,  4.],\n",
      "          ...,\n",
      "          [ 5.,  4.,  5.,  ...,  3.,  4.,  4.],\n",
      "          [ 3.,  3.,  3.,  ...,  3.,  4.,  5.],\n",
      "          [ 4.,  5.,  5.,  ...,  5.,  5.,  5.]],\n",
      "\n",
      "         [[ 3.,  3.,  5.,  ...,  4.,  0.,  1.],\n",
      "          [ 4.,  4.,  4.,  ...,  2.,  0.,  1.],\n",
      "          [ 4.,  4.,  4.,  ...,  2.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 2.,  0.,  0.,  ...,  0.,  1.,  0.],\n",
      "          [ 3.,  0.,  0.,  ...,  1.,  0.,  0.],\n",
      "          [ 2.,  0.,  1.,  ...,  1.,  0.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[ 2.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 0.,  0.,  1.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  2.,  1.],\n",
      "          [ 1.,  2.,  2.,  ...,  2.,  2.,  1.]],\n",
      "\n",
      "         [[ 0.,  5.,  1.,  ...,  1.,  2.,  2.],\n",
      "          [ 0.,  2.,  1.,  ...,  1.,  1.,  2.],\n",
      "          [ 0.,  2.,  0.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  1.,  0.,  2.],\n",
      "          [ 0.,  1.,  1.,  ...,  1.,  0.,  2.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 5.,  5.,  0.,  ...,  0.,  2.,  0.],\n",
      "          [ 4.,  3.,  0.,  ...,  0.,  3.,  2.],\n",
      "          [ 4.,  4.,  0.,  ...,  0.,  2.,  0.],\n",
      "          ...,\n",
      "          [ 5.,  2.,  2.,  ...,  2.,  2.,  2.],\n",
      "          [ 7.,  2.,  2.,  ...,  2.,  2.,  1.],\n",
      "          [ 4.,  3.,  2.,  ...,  2.,  2.,  2.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  5.,  ...,  5.,  6.,  4.],\n",
      "          [ 0.,  1.,  3.,  ...,  2.,  3.,  2.],\n",
      "          ...,\n",
      "          [ 1.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 0.,  1.,  0.,  ...,  0.,  1.,  0.]],\n",
      "\n",
      "         [[ 5.,  8.,  2.,  ...,  1.,  1.,  0.],\n",
      "          [ 4.,  7.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 4.,  8.,  1.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 3.,  3.,  3.,  ...,  3.,  4.,  4.],\n",
      "          [ 3.,  4.,  3.,  ...,  3.,  4.,  5.],\n",
      "          [ 4.,  5.,  5.,  ...,  5.,  5.,  5.]],\n",
      "\n",
      "         [[ 4.,  1.,  0.,  ...,  4.,  4.,  3.],\n",
      "          [ 4.,  2.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 4.,  0.,  0.,  ...,  1.,  1.,  0.],\n",
      "          ...,\n",
      "          [ 3.,  0.,  2.,  ...,  1.,  0.,  1.],\n",
      "          [ 3.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 2.,  0.,  1.,  ...,  1.,  0.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[ 2.,  2.,  2.,  ...,  2.,  2.,  1.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  1.,  1.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 1.,  1.,  2.,  ...,  0.,  0.,  0.],\n",
      "          [ 1.,  2.,  2.,  ...,  1.,  1.,  0.]],\n",
      "\n",
      "         [[ 0.,  3.,  2.,  ...,  2.,  2.,  2.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  1.],\n",
      "          [ 0.,  1.,  1.,  ...,  1.,  0.,  2.],\n",
      "          ...,\n",
      "          [ 0.,  1.,  1.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  1.,  0.,  ...,  0.,  0.,  1.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 6.,  3.,  3.,  ...,  3.,  2.,  2.],\n",
      "          [ 5.,  2.,  2.,  ...,  2.,  2.,  2.],\n",
      "          [ 5.,  2.,  2.,  ...,  2.,  2.,  1.],\n",
      "          ...,\n",
      "          [ 5.,  2.,  1.,  ...,  2.,  1.,  1.],\n",
      "          [ 7.,  2.,  2.,  ...,  2.,  2.,  1.],\n",
      "          [ 4.,  3.,  2.,  ...,  2.,  3.,  2.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 1.,  0.,  0.,  ...,  0.,  0.,  1.],\n",
      "          ...,\n",
      "          [ 1.,  0.,  0.,  ...,  0.,  0.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  0.,  0.,  1.],\n",
      "          [ 0.,  1.,  1.,  ...,  1.,  0.,  1.]],\n",
      "\n",
      "         [[ 4.,  3.,  3.,  ...,  3.,  4.,  4.],\n",
      "          [ 3.,  3.,  2.,  ...,  2.,  4.,  4.],\n",
      "          [ 3.,  3.,  3.,  ...,  3.,  4.,  4.],\n",
      "          ...,\n",
      "          [ 3.,  3.,  3.,  ...,  2.,  2.,  3.],\n",
      "          [ 3.,  4.,  4.,  ...,  2.,  2.,  2.],\n",
      "          [ 4.,  5.,  4.,  ...,  2.,  3.,  1.]],\n",
      "\n",
      "         [[ 3.,  1.,  2.,  ...,  2.,  1.,  1.],\n",
      "          [ 3.,  1.,  1.,  ...,  1.,  0.,  1.],\n",
      "          [ 3.,  0.,  1.,  ...,  1.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 3.,  0.,  1.,  ...,  2.,  2.,  2.],\n",
      "          [ 3.,  0.,  0.,  ...,  2.,  2.,  1.],\n",
      "          [ 2.,  0.,  1.,  ...,  2.,  2.,  2.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 2.,  2.,  2.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  4.,  2.,  0.],\n",
      "          [ 0.,  0.,  1.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  1.,  1.,  0.],\n",
      "          [ 1.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 1.,  2.,  2.,  ...,  5.,  5.,  3.]],\n",
      "\n",
      "         [[ 0.,  4.,  5.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  1.,  ...,  2.,  0.,  1.],\n",
      "          [ 0.,  2.,  2.,  ...,  1.,  0.,  2.],\n",
      "          ...,\n",
      "          [ 0.,  1.,  1.,  ...,  2.,  1.,  2.],\n",
      "          [ 0.,  1.,  0.,  ...,  1.,  0.,  2.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 6.,  2.,  6.,  ...,  0.,  0.,  6.],\n",
      "          [ 5.,  2.,  4.,  ...,  1.,  0.,  6.],\n",
      "          [ 5.,  3.,  5.,  ...,  1.,  0.,  6.],\n",
      "          ...,\n",
      "          [ 5.,  2.,  2.,  ...,  1.,  1.,  2.],\n",
      "          [ 7.,  2.,  2.,  ...,  1.,  0.,  0.],\n",
      "          [ 4.,  3.,  2.,  ...,  3.,  3.,  3.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  1.,  ...,  2.,  1.,  2.],\n",
      "          [ 1.,  1.,  1.,  ...,  0.,  0.,  2.],\n",
      "          ...,\n",
      "          [ 1.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 1.,  1.,  0.,  ...,  0.,  0.,  2.],\n",
      "          [ 0.,  1.,  1.,  ...,  1.,  1.,  0.]],\n",
      "\n",
      "         [[ 4.,  4.,  6.,  ...,  0.,  0.,  3.],\n",
      "          [ 3.,  4.,  7.,  ...,  1.,  0.,  3.],\n",
      "          [ 3.,  5.,  7.,  ...,  4.,  0.,  3.],\n",
      "          ...,\n",
      "          [ 3.,  4.,  3.,  ...,  4.,  5.,  5.],\n",
      "          [ 3.,  4.,  5.,  ...,  1.,  0.,  1.],\n",
      "          [ 4.,  5.,  5.,  ...,  5.,  6.,  6.]],\n",
      "\n",
      "         [[ 3.,  0.,  1.,  ...,  4.,  6.,  6.],\n",
      "          [ 3.,  0.,  1.,  ...,  2.,  1.,  2.],\n",
      "          [ 3.,  0.,  1.,  ...,  3.,  2.,  3.],\n",
      "          ...,\n",
      "          [ 3.,  0.,  2.,  ...,  1.,  0.,  0.],\n",
      "          [ 3.,  0.,  0.,  ...,  1.,  2.,  2.],\n",
      "          [ 2.,  0.,  1.,  ...,  0.,  0.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[ 2.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  2.,  ...,  0.,  0.,  1.],\n",
      "          [ 0.,  1.,  1.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 1.,  2.,  1.,  ...,  0.,  0.,  0.],\n",
      "          [ 2.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  5.,  0.,  ...,  0.,  1.,  0.],\n",
      "          [ 0.,  2.,  0.,  ...,  0.,  0.,  1.],\n",
      "          [ 0.,  2.,  0.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  2.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  3.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 5.,  6.,  0.,  ...,  0.,  2.,  0.],\n",
      "          [ 4.,  4.,  0.,  ...,  1.,  2.,  0.],\n",
      "          [ 5.,  5.,  0.,  ...,  1.,  2.,  0.],\n",
      "          ...,\n",
      "          [ 5.,  5.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 5.,  3.,  0.,  ...,  0.,  1.,  0.],\n",
      "          [ 4.,  4.,  0.,  ...,  1.,  2.,  0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 0.,  1.,  3.,  ...,  3.,  4.,  4.],\n",
      "          [ 0.,  2.,  3.,  ...,  2.,  2.,  3.],\n",
      "          ...,\n",
      "          [ 0.,  2.,  2.,  ...,  2.,  3.,  3.],\n",
      "          [ 0.,  2.,  3.,  ...,  4.,  3.,  3.],\n",
      "          [ 0.,  1.,  4.,  ...,  3.,  3.,  5.]],\n",
      "\n",
      "         [[ 5.,  7.,  2.,  ...,  2.,  1.,  0.],\n",
      "          [ 4.,  7.,  1.,  ...,  1.,  1.,  0.],\n",
      "          [ 4.,  7.,  1.,  ...,  1.,  1.,  0.],\n",
      "          ...,\n",
      "          [ 4.,  8.,  0.,  ...,  2.,  1.,  0.],\n",
      "          [ 4.,  8.,  1.,  ...,  1.,  1.,  0.],\n",
      "          [ 5., 10.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 4.,  0.,  0.,  ...,  4.,  4.,  4.],\n",
      "          [ 4.,  2.,  0.,  ...,  2.,  2.,  0.],\n",
      "          [ 3.,  0.,  0.,  ...,  2.,  2.,  1.],\n",
      "          ...,\n",
      "          [ 4.,  0.,  0.,  ...,  1.,  2.,  2.],\n",
      "          [ 4.,  0.,  0.,  ...,  2.,  2.,  1.],\n",
      "          [ 2.,  0.,  0.,  ...,  4.,  4.,  4.]]],\n",
      "\n",
      "\n",
      "        [[[ 0.,  0.,  0.,  ...,  1.,  2.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  0.,  0.,  0.],\n",
      "          [ 1.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  2.,  1.],\n",
      "          [ 1.,  2.,  2.,  ...,  2.,  2.,  1.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  5.,  5.,  2.],\n",
      "          [ 0.,  1.,  0.,  ...,  2.,  0.,  1.],\n",
      "          [ 0.,  0.,  0.,  ...,  3.,  2.,  2.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  1.,  1.,  2.],\n",
      "          [ 0.,  1.,  1.,  ...,  1.,  0.,  2.],\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  1.,  ..., 11.,  2.,  2.],\n",
      "          [ 0.,  1.,  1.,  ..., 11.,  2.,  2.],\n",
      "          [ 0.,  0.,  1.,  ..., 10.,  2.,  2.],\n",
      "          ...,\n",
      "          [ 5.,  2.,  2.,  ...,  2.,  2.,  2.],\n",
      "          [ 7.,  2.,  2.,  ...,  2.,  2.,  1.],\n",
      "          [ 4.,  3.,  2.,  ...,  2.,  2.,  2.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 2.,  4.,  4.,  ...,  0.,  0.,  0.],\n",
      "          [ 2.,  2.,  2.,  ...,  1.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 0.,  1.,  0.,  ...,  0.,  1.,  0.]],\n",
      "\n",
      "         [[ 2.,  1.,  1.,  ...,  6.,  4.,  4.],\n",
      "          [ 1.,  0.,  1.,  ...,  5.,  5.,  4.],\n",
      "          [ 1.,  1.,  1.,  ...,  5.,  5.,  4.],\n",
      "          ...,\n",
      "          [ 3.,  3.,  3.,  ...,  3.,  4.,  4.],\n",
      "          [ 3.,  4.,  3.,  ...,  3.,  4.,  5.],\n",
      "          [ 4.,  5.,  5.,  ...,  5.,  5.,  5.]],\n",
      "\n",
      "         [[ 3.,  4.,  5.,  ...,  4.,  1.,  1.],\n",
      "          [ 3.,  0.,  1.,  ...,  2.,  0.,  1.],\n",
      "          [ 2.,  1.,  2.,  ...,  2.,  0.,  0.],\n",
      "          ...,\n",
      "          [ 3.,  1.,  1.,  ...,  1.,  0.,  0.],\n",
      "          [ 3.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "          [ 2.,  0.,  1.,  ...,  1.,  0.,  1.]]]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_bit = 4\n",
    "x = save_output.outputs[2][0]  # input of the 2nd conv layer\n",
    "x_alpha  = model.layer1[0].conv2.act_alpha\n",
    "x_delta = x_alpha/(2**x_bit-1)\n",
    "\n",
    "act_quant_fn = act_quantization(x_bit) # define the quantization function\n",
    "x_q = act_quant_fn(x, x_alpha)         # create the quantized value for x\n",
    "\n",
    "x_int = x_q/x_delta\n",
    "print(x_int) # you should see clean integer numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ranging-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_int = torch.nn.Conv2d(in_channels = 8, out_channels=8, kernel_size = 3, padding=1, bias = False)\n",
    "conv_int.weight = torch.nn.parameter.Parameter(weight_int)\n",
    "relu = nn.ReLU()\n",
    "bn = nn.BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True).to(device)\n",
    "output_int = conv_int(x_int)\n",
    "output_int = output_int*w_delta*x_delta+save_output.outputs[1][0]\n",
    "output_recovered = relu(output_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d5639a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4376e-07, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "difference = abs(save_output.outputs[3][0] - output_recovered )\n",
    "print(difference.mean())  ## It should be small, e.g.,2.3 in my trainned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### input floating number / weight quantized version\n",
    "\n",
    "conv_ref = torch.nn.Conv2d(in_channels = 16, out_channels=16, kernel_size = 3, bias = False)\n",
    "conv_ref.weight = model.layer1[0].conv1.weight_q \n",
    "\n",
    "output_ref = conv_ref(x)\n",
    "print(output_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### input floating number / weight floating number version\n",
    "\n",
    "conv_ref = torch.nn.Conv2d(in_channels = 16, out_channels=16, kernel_size = 3, bias = False)\n",
    "weight = model.layer1[0].conv1.weight\n",
    "mean = weight.data.mean()\n",
    "std = weight.data.std()\n",
    "conv_ref.weight = torch.nn.parameter.Parameter(weight.add(-mean).div(std))\n",
    "\n",
    "output_ref = conv_ref(x)\n",
    "print(output_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = abs( output_ref - output_recovered )\n",
    "print(difference.mean())  ## It should be small, e.g.,2.3 in my trainned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-significance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-witch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-barbados",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-serbia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
